import { NextRequest } from 'next/server';
import { 
  validateApiKey, 
  extractApiKey, 
  isAuthEnabled, 
  createAuthResponse 
} from '@/utils/auth';
import {
  OpenAICompletionRequest,
  convertOpenAIToLangChain,
  createOpenAIResponse,
  formatStreamChunk,
  createStreamEnd,
  detectIntentFromRequest,
  detectModelSwitchRequest,
  selectBestModelForAuto
} from '@/utils/openai-compat';
import { routeRequest, RoutingRequest } from '@/utils/unified-router';

// OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹
export async function POST(req: NextRequest) {
  try {
    console.log('OpenAI Compatible API request received');

    // APIå¯†é’¥éªŒè¯
    if (isAuthEnabled()) {
      const apiKey = extractApiKey(req);
      const keyInfo = validateApiKey(apiKey);
      
      if (!keyInfo.isValid) {
        console.log('Invalid API key provided:', apiKey?.substring(0, 10) + '...');
        return createAuthResponse('Invalid API key provided');
      }
      
      console.log(`Valid API key used: ${keyInfo.isAdmin ? 'Admin' : 'User'} key`);
    }

    // è§£æè¯·æ±‚ä½“
    const body: OpenAICompletionRequest = await req.json();
    
    if (!body.messages || !Array.isArray(body.messages) || body.messages.length === 0) {
      return new Response(
        JSON.stringify({
          error: {
            message: 'Messages array is required and cannot be empty',
            type: 'invalid_request_error',
            code: 'missing_messages'
          }
        }),
        { 
          status: 400, 
          headers: { 'Content-Type': 'application/json' } 
        }
      );
    }

    console.log(`OpenAI API - Model: ${body.model}, Messages: ${body.messages.length}, Stream: ${body.stream}`);

    // ğŸš€ ä½¿ç”¨ç»Ÿä¸€æ™ºèƒ½è·¯ç”±å™¨è¿›è¡Œæ¨¡å‹é€‰æ‹©
    const routingRequest: RoutingRequest = {
      messages: body.messages,
      userIntent: body.model !== 'auto' ? body.model : undefined,
      context: {
        taskType: 'chat',
        language: 'auto'
      },
      tools: body.tools,
      temperature: body.temperature,
      stream: body.stream
    };

    // è°ƒç”¨ç»Ÿä¸€è·¯ç”±å™¨è¿›è¡Œæ™ºèƒ½é€‰æ‹©
    const routingDecision = await routeRequest(routingRequest);
    
    console.log(`ğŸ¯ ç»Ÿä¸€è·¯ç”±å™¨å†³ç­–:`);
    console.log(`  - é€‰æ‹©æ¨¡å‹: ${routingDecision.selectedModel}`);
    console.log(`  - ç½®ä¿¡åº¦: ${routingDecision.confidence}`);
    console.log(`  - ç­–ç•¥: ${routingDecision.metadata.routingStrategy}`);
    console.log(`  - æ¨ç†: ${routingDecision.reasoning}`);
    
    // æ›´æ–°è¯·æ±‚ä¸­çš„æ¨¡å‹
    body.model = routingDecision.selectedModel;
    
    // è½¬æ¢ä¸ºLangChainæ ¼å¼
    const langchainRequest = convertOpenAIToLangChain(body);
    
    // æ™ºèƒ½è·¯ç”± - æ ¹æ®è¯·æ±‚å†…å®¹é€‰æ‹©åˆé€‚çš„ç«¯ç‚¹
    const targetEndpoint = await detectIntentFromRequest(body);
    console.log(`Routing to endpoint: ${targetEndpoint}`);

    // æ„å»ºå†…éƒ¨è¯·æ±‚
    const internalUrl = new URL(targetEndpoint, req.url);
    const internalRequest = new Request(internalUrl.toString(), {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'X-OpenAI-Compat': 'true', // æ·»åŠ æ ‡è¯†ï¼Œè®©å†…éƒ¨APIçŸ¥é“è¿™æ˜¯æ¥è‡ªOpenAIå…¼å®¹å±‚çš„è¯·æ±‚
        // ä¼ é€’åŸå§‹è¯·æ±‚çš„ä¸€äº›å¤´ä¿¡æ¯
        'X-Forwarded-For': req.headers.get('X-Forwarded-For') || '',
        'User-Agent': req.headers.get('User-Agent') || '',
      },
      body: JSON.stringify(langchainRequest)
    });

    // è°ƒç”¨å†…éƒ¨API
    const internalResponse = await fetch(internalRequest);
    
    if (!internalResponse.ok) {
      console.error(`Internal API error: ${internalResponse.status}`);
      return new Response(
        JSON.stringify({
          error: {
            message: 'Internal server error',
            type: 'server_error',
            code: 'internal_error'
          }
        }),
        { 
          status: 500, 
          headers: { 'Content-Type': 'application/json' } 
        }
      );
    }

    // è·å–æ¨¡å‹ä¿¡æ¯
    let modelUsed = internalResponse.headers.get('X-Model-Used') || body.model || 'auto';
    let actualModel = modelUsed === 'auto' ? 'langchain-auto' : modelUsed;

    // æ£€æµ‹æ¨¡å‹åˆ‡æ¢è¯·æ±‚
    const lastMessage = body.messages[body.messages.length - 1];
    const messageContent = Array.isArray(lastMessage.content) ? lastMessage.content.map(c => typeof c === 'string' ? c : c.text).join('') : lastMessage.content;
    const switchModel = detectModelSwitchRequest(messageContent);
    if (switchModel) {
      console.log(`Detected model switch request: ${switchModel}`);
      actualModel = switchModel;
    }

    // å¤„ç†æµå¼å“åº”
    if (body.stream !== false) {
      console.log('Streaming OpenAI compatible response');
      
      const encoder = new TextEncoder();
      const reader = internalResponse.body?.getReader();
      
      if (!reader) {
        throw new Error('Failed to get response reader');
      }

      const readableStream = new ReadableStream({
        async start(controller) {
          try {
            let buffer = '';
            
            // æ·»åŠ æ¨¡å‹ç‰¹å®šçš„å¤„ç†é€»è¾‘
            const isGeminiModel = actualModel.includes('gemini');
            let modelInfoSent = false;
            let contentBuffer = '';
            
            while (true) {
              const { done, value } = await reader.read();
              
              if (done) {
                // å¤„ç†å‰©ä½™çš„buffer
                if (buffer.trim()) {
                  if (isGeminiModel && !modelInfoSent) {
                    // å¦‚æœæ˜¯ Gemini æ¨¡å‹ä¸”è¿˜æ²¡å‘é€æ¨¡å‹ä¿¡æ¯ï¼Œä¸€èµ·å‘é€
                    const combinedContent = contentBuffer + buffer;
                    if (combinedContent.trim()) {
                      const openaiChunk = createOpenAIResponse(combinedContent, actualModel, false, true);
                      controller.enqueue(encoder.encode(formatStreamChunk(openaiChunk)));
                    }
                  } else {
                    const openaiChunk = createOpenAIResponse(buffer, actualModel, false, true);
                    controller.enqueue(encoder.encode(formatStreamChunk(openaiChunk)));
                  }
                }
                
                // å‘é€æœ€åçš„å®Œæˆå—
                const finalChunk = createOpenAIResponse('', actualModel, true, true);
                controller.enqueue(encoder.encode(formatStreamChunk(finalChunk)));
                controller.enqueue(encoder.encode(createStreamEnd()));
                break;
              }
              
              // è§£ç æ•°æ®å—
              const chunk = new TextDecoder().decode(value);
              buffer += chunk;
              
              // å¤„ç†å¯èƒ½çš„éƒ¨åˆ†æ•°æ®
              const lines = buffer.split('\n');
              buffer = lines.pop() || ''; // ä¿ç•™æœ€åä¸€ä¸ªå¯èƒ½ä¸å®Œæ•´çš„è¡Œ
              
              for (const line of lines) {
                if (line.trim()) {
                  // å¯¹äº Gemini æ¨¡å‹ï¼Œç‰¹æ®Šå¤„ç†æ¨¡å‹ä¿¡æ¯
                  if (isGeminiModel) {
                    if (line.includes('ğŸ¤–') || line.includes('---')) {
                      // ç¼“å­˜æ¨¡å‹ä¿¡æ¯ï¼Œä¸ç«‹å³å‘é€
                      contentBuffer += line + '\n';
                      continue;
                    }
                    
                    // å½“æ”¶åˆ°å®é™…å†…å®¹æ—¶ï¼Œä¸€èµ·å‘é€æ¨¡å‹ä¿¡æ¯å’Œå†…å®¹
                    if (!modelInfoSent && !line.includes('ğŸ¤–') && !line.includes('---')) {
                      const combinedContent = contentBuffer + line + '\n';
                      const openaiChunk = createOpenAIResponse(combinedContent, actualModel, false, true);
                      controller.enqueue(encoder.encode(formatStreamChunk(openaiChunk)));
                      modelInfoSent = true;
                      contentBuffer = '';
                    } else if (modelInfoSent) {
                      // åç»­å†…å®¹æ­£å¸¸å‘é€
                      const openaiChunk = createOpenAIResponse(line + '\n', actualModel, false, true);
                      controller.enqueue(encoder.encode(formatStreamChunk(openaiChunk)));
                    }
                  } else {
                    // é Gemini æ¨¡å‹çš„æ­£å¸¸å¤„ç†
                    const openaiChunk = createOpenAIResponse(line + '\n', actualModel, false, true);
                    controller.enqueue(encoder.encode(formatStreamChunk(openaiChunk)));
                  }
                }
              }
            }
            
            controller.close();
          } catch (error) {
            console.error('Streaming error:', error);
            controller.error(error);
          }
        }
      });

      return new Response(readableStream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        }
      });
    } else {
      // éæµå¼å“åº”
      console.log('Non-streaming OpenAI compatible response');
      
      const responseText = await internalResponse.text();
      const openaiResponse = createOpenAIResponse(responseText, actualModel, true, false);
      
      return new Response(JSON.stringify(openaiResponse), {
        headers: {
          'Content-Type': 'application/json',
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        }
      });
    }

  } catch (error) {
    console.error('OpenAI Compatible API error:', error);
    
    return new Response(
      JSON.stringify({
        error: {
          message: error instanceof Error ? error.message : 'Unknown error occurred',
          type: 'server_error',
          code: 'internal_error'
        }
      }),
      { 
        status: 500, 
        headers: { 'Content-Type': 'application/json' } 
      }
    );
  }
}

// å¤„ç†OPTIONSè¯·æ±‚ (CORSé¢„æ£€)
export async function OPTIONS(req: NextRequest) {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type, Authorization, X-API-Key',
      'Access-Control-Max-Age': '86400',
    }
  });
}
