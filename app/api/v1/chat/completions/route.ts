import { NextRequest, NextResponse } from 'next/server';
import { 
  validateApiKey, 
  extractApiKey, 
  isAuthEnabled, 
  createAuthResponse 
} from '@/utils/auth';
import {
  OpenAICompletionRequest,
  convertOpenAIToLangChain,
  createOpenAIResponse,
  formatStreamChunk,
  createStreamEnd,
  detectModelSwitchRequest,
  detectIntentFromRequest,
  selectBestModelForAuto
} from '@/utils/openai-compat';
import { routeRequest as route } from '@/utils/unified-router';
import { handleApiKeyValidation } from './helpers';
import { routeRequest, RoutingRequest } from '@/utils/unified-router';
import { wrapWithErrorHandling } from '@/utils/errorHandler';
import { sendEvent } from '@/utils/langfuseClient';

// OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹
export async function POST(req: NextRequest) {
  return wrapWithErrorHandling("v1_chat_completions_POST", async () => {
    sendEvent({ event: 'request_start', properties: { endpoint: 'v1_chat_completions_POST' }, timestamp: new Date().toISOString() });

    // APIå¯†é’¥éªŒè¯
    const authResponse = await handleApiKeyValidation(req);
    if (authResponse) {
      return authResponse;
    }

    // è§£æè¯·æ±‚ä½“
    const body: OpenAICompletionRequest = await req.json();
    
    if (!body.messages || !Array.isArray(body.messages) || body.messages.length === 0) {
      return new Response(
        JSON.stringify({
          error: {
            message: 'Messages array is required and cannot be empty',
            type: 'invalid_request_error',
            code: 'missing_messages'
          }
        }),
        { 
          status: 400, 
          headers: { 'Content-Type': 'application/json' } 
        }
      );
    }

    console.log(`OpenAI API - Model: ${body.model}, Messages: ${body.messages.length}, Stream: ${body.stream}`);

    // ç¼“å­˜ç”¨æˆ·æ¶ˆæ¯å†…å®¹ï¼Œé¿å…é‡å¤è§£æ
    const userMessage = body.messages[body.messages.length - 1];
    const userContent = Array.isArray(userMessage.content)
      ? userMessage.content.map(c => typeof c === 'string' ? c : c.text).join('')
      : userMessage.content;

    // å…ˆæ£€æµ‹æ¨¡å‹åˆ‡æ¢æ„å›¾
    const detectedModel = detectModelSwitchRequest(userContent);
    console.log(`ğŸ” æ£€æµ‹åˆ°çš„æ¨¡å‹åˆ‡æ¢æ„å›¾: ${detectedModel}`);
    console.log(`ğŸ“ ç”¨æˆ·æ¶ˆæ¯: "${userContent}"`);

    // ç»Ÿä¸€è·¯ç”±è¯·æ±‚æ„é€ ï¼Œä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„æ¨¡å‹
    const routingRequest: RoutingRequest = {
      messages: body.messages,
      userIntent: detectedModel || (body.model !== 'auto' ? body.model : undefined),
      context: {
        taskType: 'chat',
        language: 'auto'
      },
      tools: body.tools,
      temperature: body.temperature,
      stream: body.stream
    };

    console.log(`ğŸ“¦ è·¯ç”±è¯·æ±‚ userIntent: ${routingRequest.userIntent}`);

    // è°ƒç”¨ç»Ÿä¸€è·¯ç”±å™¨è¿›è¡Œæ™ºèƒ½é€‰æ‹©
    const routingDecision = await routeRequest(routingRequest);

    sendEvent({ event: 'model_selected', properties: { model: routingDecision.selectedModel, confidence: routingDecision.confidence }, timestamp: new Date().toISOString() });

    console.log(`ğŸ¯ ç»Ÿä¸€è·¯ç”±å™¨å†³ç­–:`);
    console.log(`  - é€‰æ‹©æ¨¡å‹: ${routingDecision.selectedModel}`);
    console.log(`  - ç½®ä¿¡åº¦: ${routingDecision.confidence}`);
    console.log(`  - ç­–ç•¥: ${routingDecision.metadata.routingStrategy}`);
    console.log(`  - æ¨ç†: ${routingDecision.reasoning}`);

    // æ›´æ–°è¯·æ±‚ä¸­çš„æ¨¡å‹
    body.model = routingDecision.selectedModel;

    // è½¬æ¢ä¸ºLangChainæ ¼å¼
    const langchainRequest = convertOpenAIToLangChain(body);

    // åˆå¹¶è°ƒç”¨ detectIntentFromRequestï¼Œä¼ å…¥ç¼“å­˜çš„ bodyï¼Œé¿å…é‡å¤è§£æ
    const targetEndpoint = await detectIntentFromRequest(body);
    console.log(`[Smart Router] Routing to endpoint: ${targetEndpoint}`);
    console.log(`[Smart Router] Selected Model: ${body.model}`);

    // æ„å»ºå†…éƒ¨è¯·æ±‚
    const baseUrl = new URL(req.url).origin;
    const internalUrl = new URL(targetEndpoint, baseUrl);
    const internalRequest = new Request(internalUrl.toString(), {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'X-OpenAI-Compat': 'true', // æ·»åŠ æ ‡è¯†ï¼Œè®©å†…éƒ¨APIçŸ¥é“è¿™æ˜¯æ¥è‡ªOpenAIå…¼å®¹å±‚çš„è¯·æ±‚
        // ä¼ é€’åŸå§‹è¯·æ±‚çš„ä¸€äº›å¤´ä¿¡æ¯
        'X-Forwarded-For': req.headers.get('X-Forwarded-For') || '',
        'User-Agent': req.headers.get('User-Agent') || '',
      },
      body: JSON.stringify(langchainRequest)
    });

    const startTime = Date.now();
    // è°ƒç”¨å†…éƒ¨API
    const internalResponse = await fetch(internalRequest);
    const durationMs = Date.now() - startTime;
    sendEvent({ event: 'internal_api_call', properties: { status: internalResponse.status, durationMs }, timestamp: new Date().toISOString() });
    
    if (!internalResponse.ok) {
      const errorText = await internalResponse.text();
      console.error(`Internal API error: ${internalResponse.status}, Response: ${errorText}`);
      return new Response(
        JSON.stringify({
          error: {
            message: `Internal server error: ${internalResponse.status} - ${errorText}`,
            type: 'server_error',
            code: 'internal_error'
          }
        }),
        {
          status: 500,
          headers: { 'Content-Type': 'application/json' }
        }
      );
    }
    
    // é¢„å…ˆè§£æå†…éƒ¨å“åº”ï¼Œä»¥ä¾¿åŒæ—¶è·å–æ¨¡å‹ä¿¡æ¯å’Œå“åº”å†…å®¹
    const internalJson = await internalResponse.json();

    // ä¼˜å…ˆä½¿ç”¨å†…éƒ¨è·¯ç”±å†³ç­–çš„æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ°v1è·¯ç”±å™¨çš„åˆæ­¥å†³ç­–
    const finalModel = internalJson.routing?.model || routingDecision.selectedModel;
    console.log(`ğŸ¯ Final model used: ${finalModel}`);


    // å¤„ç†æµå¼å“åº”
    if (body.stream !== false) {
      console.log('Streaming OpenAI compatible response');
      const coreResponse = internalJson.response || JSON.stringify(internalJson);
      
      const encoder = new TextEncoder();
      
      const readableStream = new ReadableStream({
        async start(controller) {
          try {
            const decisionInfo = `ğŸ¯ ç»Ÿä¸€è·¯ç”±å™¨å†³ç­–:
  - é€‰æ‹©æ¨¡å‹: ${finalModel}
  - ç½®ä¿¡åº¦: ${routingDecision.confidence}
  - ç­–ç•¥: ${routingDecision.metadata.routingStrategy}
  - æ¨ç†: ${routingDecision.reasoning}
  - è·¯ç”±: ${targetEndpoint}

---

`;
            const initialChunk = createOpenAIResponse(decisionInfo, finalModel, false, true);
            controller.enqueue(encoder.encode(formatStreamChunk(initialChunk)));

            // æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ ¸å¿ƒå“åº”å†…å®¹
            const finalContentChunk = createOpenAIResponse(coreResponse, finalModel, false, true);
            controller.enqueue(encoder.encode(formatStreamChunk(finalContentChunk)));

            // å‘é€ç»“æŸæ ‡å¿—
            const finalChunk = createOpenAIResponse('', finalModel, true, true);
            controller.enqueue(encoder.encode(formatStreamChunk(finalChunk)));
            controller.enqueue(encoder.encode(createStreamEnd()));
            
            controller.close();
          } catch (error) {
            console.error('Streaming error:', error);
            let errorMessage = 'Unknown error';
            if (error instanceof Error) errorMessage = error.message;
            else if (typeof error === 'string') errorMessage = error;
            
            const errorChunk = createOpenAIResponse(
              JSON.stringify({
                error: {
                  message: 'Streaming error occurred: ' + errorMessage,
                  type: 'server_error',
                  code: 'streaming_error'
                }
              }),
              finalModel,
              false,
              true
            );
            controller.enqueue(encoder.encode(formatStreamChunk(errorChunk)));
            controller.enqueue(encoder.encode(createStreamEnd()));
            controller.close();
          }
        }
      });

      return new Response(readableStream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        }
      });
    } else {
      // éæµå¼å“åº”
      console.log('Non-streaming OpenAI compatible response');
      
      const coreResponse = internalJson.response || JSON.stringify(internalJson);
      const decisionInfo = `ğŸ¯ ç»Ÿä¸€è·¯ç”±å™¨å†³ç­–:
  - é€‰æ‹©æ¨¡å‹: ${finalModel}
  - ç½®ä¿¡åº¦: ${routingDecision.confidence}
  - ç­–ç•¥: ${routingDecision.metadata.routingStrategy}
  - æ¨ç†: ${routingDecision.reasoning}
  - è·¯ç”±: ${targetEndpoint}

---

`;
      const responseText = decisionInfo + coreResponse;
      const openaiResponse = createOpenAIResponse(responseText, finalModel, true, false);
      
      return new Response(JSON.stringify(openaiResponse), {
        headers: {
          'Content-Type': 'application/json',
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        }
      });
    }
  }, {
    interfaceName: "v1_chat_completions_POST",
    fallback: async () => {
      // å›é€€é€»è¾‘ç¤ºä¾‹ï¼šè¿”å›æ ‡å‡†é”™è¯¯å“åº”ï¼Œæç¤ºæœåŠ¡æš‚ä¸å¯ç”¨
      return new Response(
        JSON.stringify({
          error: {
            message: 'Service temporarily unavailable, please try again later.',
            type: 'server_error',
            code: 'service_unavailable'
          }
        }),
        {
          status: 503,
          headers: { 'Content-Type': 'application/json' }
        }
      );
    }
  });
}

// å¤„ç†OPTIONSè¯·æ±‚ (CORSé¢„æ£€)
export async function OPTIONS(req: NextRequest) {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type, Authorization, X-API-Key',
      'Access-Control-Max-Age': '86400',
    }
  });
}