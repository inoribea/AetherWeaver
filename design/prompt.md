    from __future__ import annotations
    
    import os
    import json
    from typing import Any, Dict, List, Union, Optional
    from datetime import datetime
    
    from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough
    
    from langflow.custom.custom_component.component import Component
    from langflow.io import MessageInput, Output, MultilineInput, StrInput, BoolInput, DropdownInput
    from langflow.schema.message import Message
    
    class AgentPromptComponent(Component, Runnable):
        display_name = "Agent Prompt (LCEL/LangChain.js Compatible)"
        description = "LCEL-compatible agent prompt with memory, tools, document context support and LangChain.js export"
        icon = "cpu"
        name = "AgentPromptComponent"
    
        inputs = [
            MessageInput(
                name="input",
                display_name="User Input",
                info="User's input message (supports routing metadata)",
                required=True,
            ),
            MessageInput(
                name="memory",
                display_name="Memory",
                info="Chat history from LangFlow ChatMemory or custom memory component",
                required=False,
            ),
            MessageInput(
                name="tools_results",
                display_name="Tools Results",
                info="Results from tool execution",
                required=False,
            ),
            MessageInput(
                name="context_documents",
                display_name="Context Documents",
                info="Retrieved documents from vector store",
                required=False,
            ),
            # æ–°å¢è·¯ç”±ä¸Šä¸‹æ–‡è¾“å…¥
            MessageInput(
                name="routing_context",
                display_name="Routing Context",
                info="Routing information from SmartRouting component",
                required=False,
            ),
            # æ–°å¢å…¼å®¹æ€§å¼€å…³
            BoolInput(
                name="vercel_mode",
                display_name="Vercel Environment Mode",
                value=False,
                info="Enable Vercel environment variable support for deployment",
                advanced=True,
            ),
            BoolInput(
                name="langchainjs_export",
                display_name="LangChain.js Export Mode",
                value=False,
                info="Enable LangChain.js compatible configuration and export",
                advanced=True,
            ),
            BoolInput(
                name="flatten_metadata",
                display_name="Flatten Metadata",
                value=True,
                info="Use flattened metadata structure for LCEL compatibility",
                advanced=True,
            ),
            BoolInput(
                name="enable_runnable_interface",
                display_name="Enable Runnable Interface",
                value=True,
                info="Make component fully LCEL Runnable compatible",
                advanced=True,
            ),
            BoolInput(
                name="adaptive_prompt_generation",
                display_name="Adaptive Prompt Generation",
                value=True,
                info="Dynamically adapt prompt based on routing context",
                advanced=False,
            ),
            BoolInput(
                name="memory_integration",
                display_name="Memory Integration",
                value=True,
                info="Automatically integrate with LangFlow ChatMemory format",
                advanced=False,
            ),
            DropdownInput(
                name="prompt_mode",
                display_name="Prompt Mode",
                options=["adaptive", "basic", "enhanced", "rag", "agent"],
                value="adaptive",
                info="Prompt generation mode",
                real_time_refresh=True,
            ),
            MultilineInput(
                name="system_template",
                display_name="System Template",
                info="Base agent system message template (will be enhanced with routing and memory)",
                value="""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIä»£ç†ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·å’ŒçŸ¥è¯†åº“æ¥å¸®åŠ©ç”¨æˆ·ã€‚
    
    å¯¹è¯å†å²:
    {memory}
    
    å·¥å…·æ‰§è¡Œç»“æœ:
    {tools_results}
    
    çŸ¥è¯†åº“æ–‡æ¡£:
    {context_documents}
    
    å·¥ä½œæŒ‡å—ï¼š
    1. ä»”ç»†åˆ†æç”¨æˆ·è¯·æ±‚
    2. å¦‚æœ‰å·¥å…·ç»“æœï¼Œä¼˜å…ˆåŸºäºå·¥å…·ç»“æœå›ç­”
    3. ç»“åˆçŸ¥è¯†åº“æ–‡æ¡£æä¾›è¡¥å……ä¿¡æ¯
    4. å¦‚æœå·¥å…·å’Œæ–‡æ¡£éƒ½æ— æ³•æ»¡è¶³éœ€æ±‚ï¼ŒåŸºäºä½ çš„çŸ¥è¯†å›ç­”
    5. æä¾›å‡†ç¡®ã€æœ‰ç”¨ã€å®Œæ•´çš„è§£ç­”
    
    ç”¨æˆ·è¯·æ±‚ï¼š{input}""",
                advanced=False,
            ),
            BoolInput(
                name="prioritize_tools",
                display_name="Prioritize Tools",
                info="Prioritize tool results over documents",
                value=True,
                advanced=True,
            ),
            BoolInput(
                name="include_routing_context",
                display_name="Include Routing Context",
                info="Include routing analysis in prompt",
                value=True,
                advanced=True,
            ),
            BoolInput(
                name="chinese_prompt_optimization",
                display_name="Chinese Prompt Optimization",
                info="Optimize prompts for Chinese language processing",
                value=True,
                advanced=True,
            ),
        ]
    
        outputs = [
            Output(
                display_name="LCEL Prompt Output",
                name="lcel_prompt_output",
                method="build_lcel_agent_prompt",
            ),
        ]
    
        def _load_environment_config(self) -> Dict[str, Any]:
            """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒVercelæ¨¡å¼"""
            
            default_config = {
                "prompt_templates": {
                    "adaptive": {
                        "basic": "ç®€æ´å‹å¥½çš„å¯¹è¯åŠ©æ‰‹æ¨¡å¼",
                        "enhanced": "æ·±åº¦åˆ†æå’Œä¸“ä¸šå»ºè®®æ¨¡å¼", 
                        "rag": "åŸºäºçŸ¥è¯†åº“çš„å‡†ç¡®å›ç­”æ¨¡å¼",
                        "agent": "å·¥å…·è°ƒç”¨å’Œä»»åŠ¡æ‰§è¡Œæ¨¡å¼"
                    },
                    "memory_integration": self.memory_integration,
                    "routing_integration": self.include_routing_context
                },
                "language_support": {
                    "chinese_optimization": self.chinese_prompt_optimization,
                    "auto_language_detection": True,
                    "bilingual_support": True
                },
                "lcel_configuration": {
                    "enable_runnable": self.enable_runnable_interface,
                    "enable_streaming": True,
                    "enable_async": True,
                    "enable_batching": True
                },
                "langchainjs_compatibility": {
                    "export_format": self.langchainjs_export,
                    "flatten_metadata": self.flatten_metadata,
                    "runnable_interface": self.enable_runnable_interface,
                    "memory_compatible": True
                },
                "memory_formats": {
                    "langflow_chat_memory": True,
                    "custom_memory": True,
                    "conversation_markers": ["Human:", "Assistant:", "User:", "AI:", "System:"]
                }
            }
            
            try:
                if self.vercel_mode:
                    config_str = os.getenv("LANGFLOW_AGENT_PROMPT_CONFIG", "{}")
                    vercel_config = json.loads(config_str) if config_str else {}
                    default_config.update(vercel_config)
                    
                    # Vercelç‰¹å®šé…ç½®
                    default_config["deployment"] = {
                        "env": "vercel",
                        "edge_compatible": True,
                        "region": os.getenv("VERCEL_REGION", "unknown"),
                        "function_timeout": int(os.getenv("VERCEL_FUNCTION_TIMEOUT", "30"))
                    }
                else:
                    config_str = os.getenv("AGENT_PROMPT_CONFIG", "{}")
                    env_config = json.loads(config_str) if config_str else {}
                    default_config.update(env_config)
                    
            except (json.JSONDecodeError, TypeError):
                pass
                
            return default_config
    
        def extract_routing_context(self, routing_context: Optional[Message] = None) -> Dict[str, Any]:
            """æå–è·¯ç”±ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…¼å®¹SmartRoutingè¾“å‡º"""
            
            context_info = {
                "route": "basic",
                "confidence": 0.7,
                "language": "auto",
                "is_chinese": False,
                "analysis_method": "unknown",
                "has_memory": False,
                "memory_format": "none",
                "conversation_history": "",
                "original_text": "",
                "metadata": {}
            }
            
            try:
                if not routing_context:
                    return context_info
                
                # ä»routing_contextçš„metadataæå–è·¯ç”±ä¿¡æ¯
                if hasattr(routing_context, 'metadata') and routing_context.metadata:
                    metadata = routing_context.metadata
                    context_info["metadata"] = metadata
                    
                    # æ”¯æŒæ‰å¹³åŒ–å’ŒåµŒå¥—æ ¼å¼
                    if self.flatten_metadata:
                        # æ‰å¹³åŒ–æ ¼å¼
                        context_info.update({
                            "route": metadata.get('routing_decision') or metadata.get('route', 'basic'),
                            "confidence": metadata.get('confidence', 0.7),
                            "analysis_method": metadata.get('analysis_method', 'unknown'),
                            "has_memory": metadata.get('has_memory', False),
                            "memory_format": metadata.get('memory_format', 'none'),
                            "conversation_history": metadata.get('conversation_history', ''),
                            "original_text": metadata.get('original_user_input', ''),
                            "is_chinese": self._detect_chinese(metadata.get('original_user_input', ''))
                        })
                    else:
                        # åµŒå¥—æ ¼å¼
                        memory_info = metadata.get('memory', {})
                        context_info.update({
                            "route": metadata.get('routing_decision') or metadata.get('route', 'basic'),
                            "confidence": metadata.get('confidence', 0.7),
                            "analysis_method": metadata.get('analysis_method', 'unknown'),
                            "has_memory": memory_info.get('has_memory', False),
                            "memory_format": memory_info.get('memory_format', 'none'),
                            "conversation_history": memory_info.get('conversation_history', ''),
                            "original_text": metadata.get('original_user_input', ''),
                            "is_chinese": self._detect_chinese(metadata.get('original_user_input', ''))
                        })
                
                # ä»routing_contextçš„æ–‡æœ¬æå–ä¿¡æ¯
                if hasattr(routing_context, 'text') and routing_context.text:
                    if not context_info["original_text"]:
                        context_info["original_text"] = routing_context.text
                        context_info["is_chinese"] = self._detect_chinese(routing_context.text)
                
                # è®¾ç½®è¯­è¨€
                context_info["language"] = "zh" if context_info["is_chinese"] else "en"
                
            except Exception as e:
                self.status = f"âš ï¸ Routing context extraction error: {str(e)[:30]}..."
            
            return context_info
    
        def _detect_chinese(self, text: str) -> bool:
            """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡"""
            if not text:
                return False
            
            chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
            return chinese_chars > len(text) * 0.3
    
        def extract_memory_content(self, memory: Optional[Message] = None) -> Dict[str, Any]:
            """æå–è®°å¿†å†…å®¹ï¼Œå…¼å®¹LangFlow ChatMemoryæ ¼å¼"""
            
            memory_info = {
                "content": "",
                "has_content": False,
                "format": "none",
                "conversation_rounds": 0,
                "latest_exchange": [],
                "processed_history": ""
            }
            
            try:
                if not memory:
                    return memory_info
                
                # è·å–è®°å¿†æ–‡æœ¬
                memory_text = ""
                if hasattr(memory, 'text') and memory.text:
                    memory_text = str(memory.text)
                elif hasattr(memory, 'content'):
                    memory_text = str(memory.content)
                else:
                    memory_text = str(memory)
                
                if not memory_text.strip():
                    return memory_info
                
                memory_info["content"] = memory_text
                memory_info["has_content"] = True
                
                # æ£€æµ‹è®°å¿†æ ¼å¼
                if self._is_langflow_chat_memory(memory_text):
                    memory_info["format"] = "langflow_chat_memory"
                    memory_info.update(self._parse_langflow_chat_memory(memory_text))
                elif self._is_structured_memory(memory_text):
                    memory_info["format"] = "structured_memory"
                    memory_info.update(self._parse_structured_memory(memory_text))
                else:
                    memory_info["format"] = "plain_text"
                    memory_info["processed_history"] = memory_text
                
            except Exception as e:
                self.status = f"âš ï¸ Memory extraction error: {str(e)[:30]}..."
            
            return memory_info
    
        def _is_langflow_chat_memory(self, text: str) -> bool:
            """æ£€æµ‹æ˜¯å¦ä¸ºLangFlow ChatMemoryæ ¼å¼"""
            indicators = ["Human:", "Assistant:", "User:", "AI:"]
            return any(indicator in text for indicator in indicators)
    
        def _parse_langflow_chat_memory(self, text: str) -> Dict[str, Any]:
            """è§£æLangFlow ChatMemoryæ ¼å¼"""
            lines = text.split('\n')
            conversations = []
            current_speaker = None
            current_content = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æµ‹å¯¹è¯æ ‡è®°
                if line.startswith(('Human:', 'User:')):
                    if current_speaker and current_content:
                        conversations.append({
                            "speaker": current_speaker,
                            "content": ' '.join(current_content)
                        })
                    current_speaker = "user"
                    current_content = [line.split(':', 1)[1].strip()]
                elif line.startswith(('Assistant:', 'AI:')):
                    if current_speaker and current_content:
                        conversations.append({
                            "speaker": current_speaker,
                            "content": ' '.join(current_content)
                        })
                    current_speaker = "assistant"
                    current_content = [line.split(':', 1)[1].strip()]
                else:
                    if current_content:
                        current_content.append(line)
            
            # æ·»åŠ æœ€åä¸€ä¸ªå¯¹è¯
            if current_speaker and current_content:
                conversations.append({
                    "speaker": current_speaker,
                    "content": ' '.join(current_content)
                })
            
            # æå–æœ€è¿‘çš„å¯¹è¯äº¤æ¢
            latest_exchange = conversations[-4:] if len(conversations) >= 4 else conversations
            
            # ç”Ÿæˆå¤„ç†åçš„å†å²è®°å½•
            processed_lines = []
            for conv in conversations:
                speaker_label = "ç”¨æˆ·" if conv["speaker"] == "user" else "åŠ©æ‰‹"
                processed_lines.append(f"{speaker_label}: {conv['content']}")
            
            return {
                "conversation_rounds": len([c for c in conversations if c["speaker"] == "user"]),
                "latest_exchange": latest_exchange,
                "processed_history": '\n'.join(processed_lines)
            }
    
        def _is_structured_memory(self, text: str) -> bool:
            """æ£€æµ‹æ˜¯å¦ä¸ºç»“æ„åŒ–è®°å¿†æ ¼å¼"""
            return '\n' in text and any(marker in text for marker in ["å¯¹è¯", "å†å²", "è®°å½•"])
    
        def _parse_structured_memory(self, text: str) -> Dict[str, Any]:
            """è§£æç»“æ„åŒ–è®°å¿†æ ¼å¼"""
            lines = text.split('\n')
            processed_lines = [line.strip() for line in lines if line.strip()]
            
            return {
                "conversation_rounds": len(processed_lines) // 2,  # ä¼°ç®—å¯¹è¯è½®æ¬¡
                "latest_exchange": processed_lines[-4:] if len(processed_lines) >= 4 else processed_lines,
                "processed_history": '\n'.join(processed_lines)
            }
    
        def extract_tools_results(self, tools_results: Optional[Message] = None) -> Dict[str, Any]:
            """æå–å·¥å…·æ‰§è¡Œç»“æœ"""
            
            tools_info = {
                "content": "",
                "has_results": False,
                "results_count": 0,
                "processed_results": ""
            }
            
            try:
                if not tools_results:
                    return tools_info
                
                # è·å–å·¥å…·ç»“æœæ–‡æœ¬
                if hasattr(tools_results, 'text') and tools_results.text:
                    content = str(tools_results.text)
                else:
                    content = str(tools_results)
                
                if content.strip():
                    tools_info["content"] = content
                    tools_info["has_results"] = True
                    tools_info["results_count"] = len(content.split('\n'))
                    tools_info["processed_results"] = content
                
            except Exception as e:
                self.status = f"âš ï¸ Tools results extraction error: {str(e)[:30]}..."
            
            return tools_info
    
        def extract_context_documents(self, context_documents: Optional[Message] = None) -> Dict[str, Any]:
            """æå–ä¸Šä¸‹æ–‡æ–‡æ¡£"""
            
            docs_info = {
                "content": "",
                "has_documents": False,
                "documents_count": 0,
                "processed_documents": ""
            }
            
            try:
                if not context_documents:
                    return docs_info
                
                # è·å–æ–‡æ¡£æ–‡æœ¬
                if hasattr(context_documents, 'text') and context_documents.text:
                    content = str(context_documents.text)
                elif hasattr(context_documents, 'data'):
                    # å¤„ç†ç»“æ„åŒ–æ–‡æ¡£æ•°æ®
                    data = context_documents.data
                    if isinstance(data, list):
                        doc_texts = []
                        for doc in data:
                            if hasattr(doc, 'page_content'):
                                doc_texts.append(doc.page_content)
                            elif hasattr(doc, 'text'):
                                doc_texts.append(doc.text)
                            else:
                                doc_texts.append(str(doc))
                        content = '\n\n'.join(doc_texts)
                    else:
                        content = str(data)
                else:
                    content = str(context_documents)
                
                if content.strip():
                    docs_info["content"] = content
                    docs_info["has_documents"] = True
                    docs_info["documents_count"] = len(content.split('\n\n'))
                    docs_info["processed_documents"] = content
                
            except Exception as e:
                self.status = f"âš ï¸ Documents extraction error: {str(e)[:30]}..."
            
            return docs_info
    
        def generate_adaptive_prompt(self, routing_context: Dict[str, Any], 
                                    env_config: Dict[str, Any]) -> str:
            """ç”Ÿæˆè‡ªé€‚åº”æç¤ºè¯ï¼ŒåŸºäºè·¯ç”±ä¸Šä¸‹æ–‡"""
            
            route = routing_context["route"]
            is_chinese = routing_context["is_chinese"]
            confidence = routing_context["confidence"]
            
            # è·å–åŸºç¡€æ¨¡æ¿
            base_template = self.system_template
            
            # æ ¹æ®è·¯ç”±ç±»å‹è°ƒæ•´æç¤ºè¯
            if route == "basic":
                if is_chinese:
                    mode_instruction = """
    å½“å‰æ¨¡å¼ï¼šåŸºç¡€å¯¹è¯æ¨¡å¼
    - æä¾›ç®€æ´ã€ç›´æ¥ã€å‹å¥½çš„å›ç­”
    - ä½¿ç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„è¯­è¨€
    - é¿å…è¿‡äºæŠ€æœ¯æ€§çš„å†…å®¹
    - ä¿æŒå¯¹è¯çš„è½»æ¾æ°›å›´"""
                else:
                    mode_instruction = """
    Current Mode: Basic Conversation Mode
    - Provide concise, direct, and friendly responses
    - Use natural, easy-to-understand language
    - Avoid overly technical content
    - Maintain a relaxed conversational atmosphere"""
                    
            elif route == "enhanced":
                if is_chinese:
                    mode_instruction = """
    å½“å‰æ¨¡å¼ï¼šå¢å¼ºåˆ†ææ¨¡å¼
    - æä¾›æ·±å…¥ã€ä¸“ä¸šã€è¯¦ç»†çš„åˆ†æ
    - è€ƒè™‘å¤šä¸ªè§’åº¦å’Œå±‚é¢
    - æä¾›å…·ä½“çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ
    - å±•ç°ä¸“ä¸šçŸ¥è¯†å’Œæ´å¯ŸåŠ›
    - ç»“æ„åŒ–ç»„ç»‡å›ç­”å†…å®¹"""
                else:
                    mode_instruction = """
    Current Mode: Enhanced Analysis Mode
    - Provide in-depth, professional, detailed analysis
    - Consider multiple angles and dimensions
    - Offer specific recommendations and solutions
    - Demonstrate professional knowledge and insights
    - Structure responses in an organized manner"""
                    
            elif route == "rag":
                if is_chinese:
                    mode_instruction = """
    å½“å‰æ¨¡å¼ï¼šçŸ¥è¯†æ£€ç´¢æ¨¡å¼
    - åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å’ŒçŸ¥è¯†åº“å›ç­”
    - ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§
    - æ˜ç¡®å¼•ç”¨ç›¸å…³æ–‡æ¡£å†…å®¹
    - å¦‚æœæ–‡æ¡£ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜ä¿¡æ¯æ¥æº
    - ä¼˜å…ˆä½¿ç”¨å·²éªŒè¯çš„çŸ¥è¯†"""
                else:
                    mode_instruction = """
    Current Mode: Knowledge Retrieval Mode
    - Answer based on provided reference documents and knowledge base
    - Ensure accuracy and reliability of information
    - Clearly reference relevant document content
    - If documents are insufficient, clearly indicate information sources
    - Prioritize verified knowledge"""
                    
            elif route == "agent":
                if is_chinese:
                    mode_instruction = """
    å½“å‰æ¨¡å¼ï¼šå·¥å…·æ‰§è¡Œæ¨¡å¼
    - å¯ä»¥ä½¿ç”¨æä¾›çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡
    - æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·
    - åŸºäºå·¥å…·æ‰§è¡Œç»“æœæä¾›ç­”æ¡ˆ
    - è§£é‡Šå·¥å…·ä½¿ç”¨çš„è¿‡ç¨‹å’Œç»“æœ
    - ç¡®ä¿ä»»åŠ¡çš„å®Œæ•´æ‰§è¡Œ"""
                else:
                    mode_instruction = """
    Current Mode: Tool Execution Mode
    - Can use provided tools to complete tasks
    - Choose appropriate tools based on user needs
    - Provide answers based on tool execution results
    - Explain the process and results of tool usage
    - Ensure complete task execution"""
            
            # ç»„åˆæœ€ç»ˆæç¤ºè¯
            adaptive_template = f"""{base_template}
    
    {mode_instruction}
    
    å½“å‰åˆ†æç½®ä¿¡åº¦: {confidence:.2f}
    è·¯ç”±åˆ†ææ–¹æ³•: {routing_context.get('analysis_method', 'unknown')}
    """
            
            return adaptive_template
    
        # LCEL Runnableæ¥å£å®ç°
        def invoke(self, input_data: Any, config: Optional[Dict] = None) -> Message:
            """LCEL Runnable.invokeæ–¹æ³•å®ç°"""
            # ä¸´æ—¶è®¾ç½®è¾“å…¥æ•°æ®
            original_input = getattr(self, 'input', None)
            self.input = input_data if isinstance(input_data, Message) else Message(text=str(input_data))
            
            try:
                result = self.build_lcel_agent_prompt()
                return result
            finally:
                # æ¢å¤åŸå§‹è¾“å…¥
                self.input = original_input
    
        async def ainvoke(self, input_data: Any, config: Optional[Dict] = None) -> Message:
            """LCEL Runnable.ainvokeå¼‚æ­¥æ–¹æ³•å®ç°"""
            return self.invoke(input_data, config)
    
        def stream(self, input_data: Any, config: Optional[Dict] = None):
            """LCEL Runnable.streamæ–¹æ³•å®ç°"""
            result = self.invoke(input_data, config)
            yield result
    
        async def astream(self, input_data: Any, config: Optional[Dict] = None):
            """LCEL Runnable.astreamå¼‚æ­¥æµæ–¹æ³•å®ç°"""
            result = await self.ainvoke(input_data, config)
            yield result
    
        def batch(self, inputs: List[Any], config: Optional[Dict] = None) -> List[Message]:
            """LCEL Runnable.batchæ–¹æ³•å®ç°"""
            return [self.invoke(input_data, config) for input_data in inputs]
    
        async def abatch(self, inputs: List[Any], config: Optional[Dict] = None) -> List[Message]:
            """LCEL Runnable.abatchå¼‚æ­¥æ‰¹å¤„ç†æ–¹æ³•å®ç°"""
            return [await self.ainvoke(input_data, config) for input_data in inputs]
    
        def build_lcel_agent_prompt(self) -> Message:
            """æ„å»ºLCELå…¼å®¹çš„Agentæç¤ºè¯"""
            try:
                # åŠ è½½ç¯å¢ƒé…ç½®
                env_config = self._load_environment_config()
                
                # æå–æ‰€æœ‰è¾“å…¥å†…å®¹
                user_input = self.input.text if hasattr(self.input, 'text') else str(self.input)
                
                # æå–è·¯ç”±ä¸Šä¸‹æ–‡
                routing_context = self.extract_routing_context(getattr(self, 'routing_context', None))
                
                # æå–è®°å¿†å†…å®¹
                memory_info = self.extract_memory_content(getattr(self, 'memory', None))
                
                # æå–å·¥å…·ç»“æœ
                tools_info = self.extract_tools_results(getattr(self, 'tools_results', None))
                
                # æå–æ–‡æ¡£ä¸Šä¸‹æ–‡
                docs_info = self.extract_context_documents(getattr(self, 'context_documents', None))
                
                # ç”Ÿæˆæç¤ºè¯æ¨¡æ¿
                if self.adaptive_prompt_generation and self.prompt_mode == "adaptive":
                    template = self.generate_adaptive_prompt(routing_context, env_config)
                else:
                    # ä½¿ç”¨å›ºå®šæ¨¡å¼çš„æ¨¡æ¿
                    template = self.system_template
                
                # æ ¹æ®ä¼˜å…ˆçº§è°ƒæ•´å†…å®¹é¡ºåº
                if self.prioritize_tools and tools_info["has_results"]:
                    template = template.replace(
                        "å¦‚æœ‰å·¥å…·ç»“æœï¼Œä¼˜å…ˆåŸºäºå·¥å…·ç»“æœå›ç­”",
                        "**ä¼˜å…ˆä½¿ç”¨å·¥å…·æ‰§è¡Œç»“æœ**ï¼Œç„¶åç»“åˆå…¶ä»–ä¿¡æ¯"
                    )
                
                # æ ¼å¼åŒ–æç¤ºè¯
                formatted_prompt = template.format(
                    input=user_input,
                    memory=memory_info["processed_history"] if memory_info["has_content"] else "æ— å†å²å¯¹è¯",
                    tools_results=tools_info["processed_results"] if tools_info["has_results"] else "æ— å·¥å…·æ‰§è¡Œç»“æœ",
                    context_documents=docs_info["processed_documents"] if docs_info["has_documents"] else "æ— ç›¸å…³æ–‡æ¡£"
                )
                
                # å¦‚æœåŒ…å«è·¯ç”±ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ è·¯ç”±ä¿¡æ¯
                if self.include_routing_context and routing_context["route"] != "basic":
                    route_info = f"""
    
    ğŸ“ è·¯ç”±åˆ†æä¿¡æ¯:
    - å¤„ç†æ¨¡å¼: {routing_context['route'].upper()}
    - åˆ†æç½®ä¿¡åº¦: {routing_context['confidence']:.2f}
    - è¯­è¨€: {'ä¸­æ–‡' if routing_context['is_chinese'] else 'English'}
    - åˆ†ææ–¹æ³•: {routing_context['analysis_method']}"""
    
                    formatted_prompt += route_info
                
                # æ„å»ºè¾“å‡ºmetadata
                metadata = self._create_prompt_metadata(
                    routing_context, memory_info, tools_info, docs_info, 
                    user_input, formatted_prompt, env_config
                )
                
                # ç”ŸæˆçŠ¶æ€æŒ‡ç¤ºå™¨
                route_indicator = f"ğŸ¯{routing_context['route'].upper()}" if routing_context['route'] != 'basic' else ""
                memory_indicator = "ğŸ’¬" if memory_info["format"] == "langflow_chat_memory" else ("ğŸ“" if memory_info["has_content"] else "ğŸ†•")
                tool_indicator = "ğŸ”§" if tools_info["has_results"] else ""
                doc_indicator = "ğŸ“š" if docs_info["has_documents"] else ""
                
                self.status = f"âœ… Prompt: {len(formatted_prompt)} å­—ç¬¦ {route_indicator}{memory_indicator}{tool_indicator}{doc_indicator}"
                
                return Message(text=formatted_prompt, metadata=metadata)
                
            except Exception as e:
                error_msg = f"Agentæç¤ºè¯æ„å»ºå¤±è´¥: {str(e)}"
                self.status = f"âŒ {error_msg[:50]}..."
                
                return Message(
                    text="Agentæç¤ºè¯æ„å»ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼ã€‚",
                    metadata={
                        "error": error_msg, 
                        "component_source": "AgentPromptComponent",
                        "fallback_mode": True
                    }
                )
    
        def _create_prompt_metadata(self, routing_context: Dict, memory_info: Dict, 
                                   tools_info: Dict, docs_info: Dict, user_input: str, 
                                   formatted_prompt: str, env_config: Dict) -> Dict[str, Any]:
            """åˆ›å»ºæç¤ºè¯metadataï¼Œæ”¯æŒLCELå’ŒLangChain.jså…¼å®¹æ€§"""
            
            if self.langchainjs_export:
                # LangChain.jså…¼å®¹æ ¼å¼ï¼ˆæ‰å¹³åŒ–ï¼‰
                return {
                    # LangChain.jsæ ¸å¿ƒå­—æ®µ
                    "type": "agent_prompt",
                    "source": "langflow",
                    "component": "AgentPromptComponent",
                    "version": "1.0",
                    "timestamp": datetime.now().isoformat(),
                    
                    # æç¤ºè¯ä¿¡æ¯
                    "prompt_type": "agent",
                    "prompt_mode": self.prompt_mode,
                    "adaptive_generation": self.adaptive_prompt_generation,
                    "template_length": len(self.system_template),
                    "prompt_length": len(formatted_prompt),
                    "user_input_length": len(user_input),
                    
                    # è·¯ç”±ä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                    "routing_route": routing_context["route"],
                    "routing_confidence": routing_context["confidence"],
                    "routing_language": routing_context["language"],
                    "routing_is_chinese": routing_context["is_chinese"],
                    "routing_analysis_method": routing_context["analysis_method"],
                    "include_routing_context": self.include_routing_context,
                    
                    # è®°å¿†ä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                    "memory_has_content": memory_info["has_content"],
                    "memory_format": memory_info["format"],
                    "memory_conversation_rounds": memory_info["conversation_rounds"],
                    "memory_integration": self.memory_integration,
                    
                    # å·¥å…·å’Œæ–‡æ¡£ä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                    "tools_has_results": tools_info["has_results"],
                    "tools_results_count": tools_info["results_count"],
                    "tools_prioritize": self.prioritize_tools,
                    "docs_has_documents": docs_info["has_documents"],
                    "docs_documents_count": docs_info["documents_count"],
                    
                    # LangChain.jså…¼å®¹æ€§æ ‡è®°
                    "lcel_compatible": True,
                    "langchainjs_ready": True,
                    "runnable_interface": self.enable_runnable_interface,
                    "runnable_type": "agent_prompt",
                    "supports_streaming": True,
                    "supports_async": True,
                    "supports_batching": True,
                    
                    # ç¯å¢ƒä¿¡æ¯
                    "env_mode": "vercel" if self.vercel_mode else "standard",
                    "flatten_metadata": self.flatten_metadata,
                    "chinese_optimization": self.chinese_prompt_optimization
                }
            elif self.flatten_metadata:
                # æ‰å¹³åŒ–æ ¼å¼
                return {
                    "prompt_type": "agent",
                    "prompt_mode": self.prompt_mode,
                    "adaptive_generation": self.adaptive_prompt_generation,
                    "has_memory": memory_info["has_content"],
                    "has_tools": tools_info["has_results"],
                    "has_documents": docs_info["has_documents"],
                    "memory_format": memory_info["format"],
                    "memory_conversation_rounds": memory_info["conversation_rounds"],
                    "tools_results_count": tools_info["results_count"],
                    "documents_count": docs_info["documents_count"],
                    "user_input_length": len(user_input),
                    "prompt_length": len(formatted_prompt),
                    "prioritize_tools": self.prioritize_tools,
                    "include_routing_context": self.include_routing_context,
                    "routing_route": routing_context["route"],
                    "routing_confidence": routing_context["confidence"],
                    "routing_language": routing_context["language"],
                    "component_source": "AgentPromptComponent",
                    "lcel_compatible": True,
                    "runnable_interface": self.enable_runnable_interface
                }
            else:
                # åµŒå¥—æ ¼å¼
                return {
                    "prompt_info": {
                        "type": "agent",
                        "mode": self.prompt_mode,
                        "adaptive_generation": self.adaptive_prompt_generation,
                        "template_length": len(self.system_template),
                        "final_length": len(formatted_prompt),
                        "user_input_length": len(user_input)
                    },
                    "routing_context": routing_context,
                    "memory_info": memory_info,
                    "tools_info": tools_info,
                    "documents_info": docs_info,
                    "configuration": {
                        "prioritize_tools": self.prioritize_tools,
                        "include_routing_context": self.include_routing_context,
                        "memory_integration": self.memory_integration,
                        "chinese_optimization": self.chinese_prompt_optimization,
                        "adaptive_prompt_generation": self.adaptive_prompt_generation
                    },
                    "lcel_compatible": True,
                    "runnable_interface": self.enable_runnable_interface,
                    "component_source": "AgentPromptComponent",
                    "timestamp": datetime.now().isoformat()
                }
    
        def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None) -> dict:
            """åŠ¨æ€æ›´æ–°é…ç½®"""
            if field_name == "prompt_mode":
                if field_value == "adaptive":
                    build_config["adaptive_prompt_generation"]["show"] = True
                    build_config["include_routing_context"]["show"] = True
                else:
                    build_config["adaptive_prompt_generation"]["show"] = False
                    build_config["include_routing_context"]["show"] = False
                    
            elif field_name == "langchainjs_export":
                if field_value:
                    build_config["enable_runnable_interface"]["show"] = True
                    build_config["flatten_metadata"]["show"] = True
                    build_config["memory_integration"]["show"] = True
                else:
                    build_config["enable_runnable_interface"]["show"] = False
                    
            elif field_name == "vercel_mode":
                if field_value:
                    build_config["enable_runnable_interface"]["value"] = True
                    build_config["flatten_metadata"]["value"] = True
                    
            elif field_name == "adaptive_prompt_generation":
                if field_value:
                    build_config["include_routing_context"]["show"] = True
                    build_config["chinese_prompt_optimization"]["show"] = True
                else:
                    build_config["include_routing_context"]["show"] = False
                    
            return build_config
    
        # LCEL Chainç»„åˆæ–¹æ³•
        def pipe(self, *others):
            """LCEL pipeæ“ä½œç¬¦æ”¯æŒ"""
            from langchain_core.runnables import RunnableSequence
            return RunnableSequence(first=self, middle=[], last=others[0] if others else RunnablePassthrough())
        
        def __or__(self, other):
            """LCEL | æ“ä½œç¬¦æ”¯æŒ"""
            return self.pipe(other)
            
        def __ror__(self, other):
            """LCEL | æ“ä½œç¬¦æ”¯æŒï¼ˆåå‘ï¼‰"""
            if hasattr(other, 'pipe'):
                return other.pipe(self)
            from langchain_core.runnables import RunnableSequence
            return RunnableSequence(first=other, middle=[], last=self)
    
        @property
        def config_specs(self):
            """LCELé…ç½®è§„èŒƒ"""
            return {
                "runnable_type": "agent_prompt",
                "supports_streaming": True,
                "supports_async": True,
                "supports_batching": True,
                "memory_compatible": True,
                "langchainjs_exportable": True
            }
    
        # å…¼å®¹åŸæœ‰æ–¹æ³•
        def build_agent_prompt(self) -> Message:
            """ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•å"""
            return self.build_lcel_agent_prompt()
        # åœ¨ä½ çš„ç»„ä»¶ä¸­æ·»åŠ memoryå¤„ç†æ–¹æ³•
        def format_memory_for_export(self, memory_message):
            """æ ¼å¼åŒ–memoryä¸ºå¯¼å‡ºå‹å¥½çš„æ ¼å¼"""
            if not memory_message:
                return None
    
            # æå–ChatMemoryå†…å®¹
            memory_content = memory_message.text if hasattr(memory_message, 'text') else str(memory_message)
    
            # è§£æå¯¹è¯å†å²
            conversations = []
            lines = memory_content.split('\n')
            for line in lines:
                if line.startswith('Human:') or line.startswith('User:'):
                    conversations.append({"role": "user", "content": line.split(':', 1)[1].strip()})
                elif line.startswith('Assistant:') or line.startswith('AI:'):
                    conversations.append({"role": "assistant", "content": line.split(':', 1)[1].strip()})
    
            return {
                "type": "chat_memory",
                "messages": conversations[-10:],  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
                "format": "langchain_compatible"
            }
        # ç»Ÿä¸€çš„memory metadataæ ¼å¼
        def create_memory_metadata(self, memory_data):
            return {
                "memory": {
                    "has_memory": bool(memory_data),
                    "format": "langflow_chat_memory",
                    "messages": memory_data.get("messages", []) if memory_data else [],
                    "export_ready": True
                },
                "langchain_export": {
                    "memory_compatible": True,
                    "vercel_ready": True
                }
            }
    