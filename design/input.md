
â€‹    
    from langflow.custom.custom_component.component import Component
    from langflow.io import (
        BoolInput,
        DropdownInput,
        FileInput,
        MessageTextInput,
        MultilineInput,
        Output,
        MessageInput,
    )
    from langflow.schema.message import Message
    from langflow.schema import Data
    from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES
    from langflow.utils.constants import (
        MESSAGE_SENDER_AI,
        MESSAGE_SENDER_NAME_USER,
        MESSAGE_SENDER_USER,
    )
    import json
    import time
    import uuid
    import os
    from typing import Dict, Any, Optional
    from datetime import datetime
    
    
    class OptimizedEnhancedChatInput(Component):
        display_name = "Enhanced Chat Input (LCEL Compatible)"
        description = "LCEL-compatible advanced chat input with intelligent routing and LangChain.js export support"
        icon = "MessagesSquare"
        name = "OptimizedEnhancedChatInput"
    
        inputs = [
            MultilineInput(
                name="input_value",
                display_name="Input Text",
                value="",
                info="Message to be processed for intelligent routing",
            ),
            DropdownInput(
                name="routing_mode",
                display_name="Routing Mode",
                options=["auto", "explicit", "hybrid"],
                value="auto",
                info="Routing analysis mode",
                advanced=False,
            ),
            DropdownInput(
                name="output_format",
                display_name="Output Format", 
                options=["lcel_compatible", "langflow_standard", "openai_format", "langchainjs_export"],
                value="lcel_compatible",
                info="Choose output format for downstream processing",
                advanced=False,
            ),
            BoolInput(
                name="enable_pre_routing",
                display_name="Enable Pre-routing Analysis",
                info="Enable intelligent pre-routing analysis",
                value=True,
                advanced=False,
            ),
            # æ–°å¢çš„å…¼å®¹æ€§å¼€å…³
            BoolInput(
                name="vercel_mode",
                display_name="Vercel Environment Mode",
                info="Enable Vercel environment variable support",
                value=False,
                advanced=True,
            ),
            BoolInput(
                name="langchainjs_export",
                display_name="LangChain.js Export Mode",
                info="Enable LangChain.js compatible export format",
                value=False,
                advanced=True,
            ),
            BoolInput(
                name="flatten_metadata",
                display_name="Flatten Metadata",
                info="Use flattened metadata structure for LCEL compatibility",
                value=True,
                advanced=True,
            ),
            MessageTextInput(
                name="explicit_route_hint",
                display_name="Explicit Route Hint",
                info="Optional explicit routing hint (basic/enhanced/rag/agent)",
                value="",
                advanced=True,
            ),
            BoolInput(
                name="should_store_message",
                display_name="Store Messages",
                info="Store the message in the history",
                value=True,
                advanced=True,
            ),
            DropdownInput(
                name="sender",
                display_name="Sender Type",
                options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],
                value=MESSAGE_SENDER_USER,
                info="Type of sender",
                advanced=True,
            ),
            MessageTextInput(
                name="sender_name",
                display_name="Sender Name",
                info="Name of the sender",
                value=MESSAGE_SENDER_NAME_USER,
                advanced=True,
            ),
            MessageTextInput(
                name="session_id",
                display_name="Session ID",
                info="Session ID for conversation tracking",
                advanced=True,
            ),
            FileInput(
                name="files",
                display_name="Files",
                file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,
                info="Files to be processed with the message",
                advanced=True,
                is_list=True,
            ),
            MessageTextInput(
                name="model_preference",
                display_name="Model Preference", 
                info="Preferred model for routing decision",
                value="gpt-4o-mini",
                advanced=True,
            ),
        ]
        
        outputs = [
            Output(
                display_name="LCEL Compatible Output",
                name="lcel_output",
                method="build_lcel_output",
            ),
        ]
    
        def build_lcel_output(self) -> Data:
            """æ„å»ºLCELå…¼å®¹çš„è¾“å‡ºï¼Œæ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼"""
            
            # åŠ è½½ç¯å¢ƒé…ç½®
            env_config = self._load_environment_config()
            
            # è¾“å…¥éªŒè¯å’Œæ ‡å‡†åŒ–
            normalized_input = self._normalize_input(self.input_value)
            if not normalized_input:
                return self._create_error_output("Empty or invalid input", env_config)
            
            try:
                # æ‰§è¡Œæ™ºèƒ½åˆ†æ
                routing_analysis = self._perform_routing_analysis(normalized_input, env_config)
                
                # æ„å»ºåŸºç¡€æ¶ˆæ¯å¯¹è±¡
                base_message = self._create_base_message(normalized_input, env_config)
                
                # æ ¹æ®è¾“å‡ºæ ¼å¼å’Œå…¼å®¹æ€§è®¾ç½®åˆ›å»ºç›¸åº”çš„æ•°æ®ç»“æ„
                if self.langchainjs_export or self.output_format == "langchainjs_export":
                    return self._create_langchainjs_output(base_message, routing_analysis, env_config)
                elif self.output_format == "lcel_compatible":
                    return self._create_lcel_output_format(base_message, routing_analysis, env_config)
                elif self.output_format == "openai_format":
                    return self._create_openai_output(base_message, routing_analysis, env_config)
                else:  # langflow_standard
                    return self._create_standard_output(base_message, routing_analysis, env_config)
                    
            except Exception as e:
                self.status = f"âŒ Error: {str(e)[:50]}..."
                return self._create_error_output(str(e), env_config)
    
        def _load_environment_config(self) -> Dict[str, Any]:
            """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒVercelæ¨¡å¼"""
            
            # é»˜è®¤é…ç½®
            default_config = {
                "session_id": f"session_{uuid.uuid4().hex[:8]}",
                "user_id": "anonymous",
                "model_configs": {
                    "basic": {"model": "gpt-4.1", "temperature": 0.1},
                    "enhanced": {"model": "gpt-4o-mini", "temperature": 0.3},
                    "rag": {"model": "gpt-4o-mini", "temperature": 0.2},
                    "agent": {"model": "gpt-4o", "temperature": 0.5}
                },
                "routing_thresholds": {
                    "confidence_threshold": 0.8,
                    "requires_llm_threshold": 0.7
                },
                "export_settings": {
                    "langchainjs_compatible": True,
                    "flatten_metadata": self.flatten_metadata,
                    "include_timestamps": True
                }
            }
            
            try:
                if self.vercel_mode:
                    # Vercelç¯å¢ƒå˜é‡æ ¼å¼
                    config_str = os.getenv("LANGFLOW_ROUTING_CONFIG", "{}")
                    vercel_config = json.loads(config_str) if config_str else {}
                    
                    # åˆå¹¶Vercelç‰¹å®šé…ç½®
                    vercel_specific = {
                        "deployment_env": "vercel",
                        "edge_function_mode": os.getenv("VERCEL_ENV") == "edge",
                        "region": os.getenv("VERCEL_REGION", "unknown")
                    }
                    default_config.update(vercel_config)
                    default_config.update(vercel_specific)
                    
                else:
                    # æ ‡å‡†ç¯å¢ƒå˜é‡
                    config_str = os.getenv("CHAT_INPUT_CONFIG", "{}")
                    env_config = json.loads(config_str) if config_str else {}
                    default_config.update(env_config)
                    
            except (json.JSONDecodeError, TypeError) as e:
                self.status = f"âš ï¸ Config parse error, using defaults: {str(e)[:30]}"
                
            return default_config
    
        def _normalize_input(self, input_text: Any) -> str:
            """è¾“å…¥æ ‡å‡†åŒ–å’ŒéªŒè¯"""
            if input_text is None:
                return ""
            
            if not isinstance(input_text, str):
                input_text = str(input_text)
            
            # åŸºç¡€æ¸…ç†
            normalized = input_text.strip()
            
            # å¤„ç†ç©ºè¾“å…¥
            if not normalized:
                return "ç©ºæ¶ˆæ¯"
                
            return normalized
    
        def _perform_routing_analysis(self, message: str, env_config: Dict[str, Any]) -> Dict[str, Any]:
            """æ‰§è¡Œæ™ºèƒ½è·¯ç”±åˆ†æï¼Œæ”¯æŒç¯å¢ƒé…ç½®"""
            
            # åŸºç¡€ç»Ÿè®¡åˆ†æ
            base_analysis = self._analyze_message_basics(message)
            
            # è·å–è·¯ç”±é˜ˆå€¼é…ç½®
            thresholds = env_config.get("routing_thresholds", {})
            confidence_threshold = thresholds.get("confidence_threshold", 0.8)
            
            # è·¯ç”±æ¨¡å¼å¤„ç†
            if self.routing_mode == "explicit" and self.explicit_route_hint:
                route_decision = self._validate_explicit_route(self.explicit_route_hint)
                confidence = 0.95
            elif self.routing_mode == "auto":
                route_decision, confidence = self._smart_route_analysis(message, base_analysis, env_config)
            else:  # hybrid
                auto_route, auto_conf = self._smart_route_analysis(message, base_analysis, env_config)
                if self.explicit_route_hint:
                    explicit_route = self._validate_explicit_route(self.explicit_route_hint)
                    route_decision = explicit_route if auto_conf < confidence_threshold else auto_route
                    confidence = max(auto_conf, 0.85)
                else:
                    route_decision, confidence = auto_route, auto_conf
            
            # æ„å»ºæ‰å¹³åŒ–çš„è·¯ç”±åˆ†æç»“æœï¼ˆLCELå…¼å®¹ï¼‰
            if self.flatten_metadata:
                routing_analysis = self._create_flattened_analysis(
                    base_analysis, route_decision, confidence, env_config
                )
            else:
                routing_analysis = self._create_nested_analysis(
                    base_analysis, route_decision, confidence, env_config
                )
            
            return routing_analysis
    
        def _create_flattened_analysis(self, base_analysis: Dict[str, Any], 
                                      route_decision: str, confidence: float, 
                                      env_config: Dict[str, Any]) -> Dict[str, Any]:
            """åˆ›å»ºæ‰å¹³åŒ–çš„åˆ†æç»“æœï¼Œç¬¦åˆLCELæ ‡å‡†"""
            
            timestamp = datetime.now().isoformat()
            requires_llm = confidence < env_config.get("routing_thresholds", {}).get("requires_llm_threshold", 0.7)
            
            return {
                # æ ¸å¿ƒè·¯ç”±ä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                "routing_decision": route_decision,
                "confidence": confidence,
                "routing_mode": self.routing_mode,
                "requires_llm_routing": requires_llm,
                "analysis_timestamp": timestamp,
                
                # åŸºç¡€åˆ†æä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                "message_length": base_analysis.get("length", 0),
                "word_count": base_analysis.get("word_count", 0),
                "is_question": base_analysis.get("is_question", False),
                "has_code": base_analysis.get("has_code", False),
                "complexity": base_analysis.get("complexity", 1),
                "needs_context": base_analysis.get("needs_context", False),
                "needs_tools": base_analysis.get("needs_tools", False),
                "language": base_analysis.get("language", "unknown"),
                
                # è·¯ç”±å…ƒæ•°æ®ï¼ˆæ‰å¹³åŒ–ï¼‰
                "priority": self._determine_priority(route_decision, confidence),
                "estimated_tokens": self._estimate_tokens(base_analysis.get("length", 0)),
                "model_config": env_config.get("model_configs", {}).get(route_decision, {}),
                
                # å¤„ç†æ ‡è®°
                "component_source": "OptimizedEnhancedChatInput",
                "lcel_compatible": True,
                "langchainjs_ready": self.langchainjs_export
            }
    
        def _create_nested_analysis(self, base_analysis: Dict[str, Any], 
                                   route_decision: str, confidence: float, 
                                   env_config: Dict[str, Any]) -> Dict[str, Any]:
            """åˆ›å»ºåµŒå¥—çš„åˆ†æç»“æœï¼ˆä¼ ç»Ÿæ ¼å¼ï¼‰"""
            
            return {
                **base_analysis,
                "routing_decision": route_decision,
                "confidence": confidence,
                "routing_mode": self.routing_mode,
                "analysis_timestamp": datetime.now().isoformat(),
                "requires_llm_routing": confidence < env_config.get("routing_thresholds", {}).get("requires_llm_threshold", 0.7),
                "routing_metadata": {
                    "input_length": base_analysis.get("length", 0),
                    "complexity_score": base_analysis.get("complexity", 1),
                    "priority": self._determine_priority(route_decision, confidence),
                    "estimated_tokens": self._estimate_tokens(base_analysis.get("length", 0)),
                    "model_config": env_config.get("model_configs", {}).get(route_decision, {})
                }
            }
    
        def _smart_route_analysis(self, message: str, base_analysis: Dict[str, Any], 
                                 env_config: Dict[str, Any]) -> tuple:
            """æ™ºèƒ½è·¯ç”±åˆ†æï¼Œæ”¯æŒç¯å¢ƒé…ç½®"""
            
            text_lower = message.lower()
            
            # ä»ç¯å¢ƒé…ç½®è·å–è·¯ç”±æ¡ä»¶ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤
            route_conditions = env_config.get("route_conditions", {
                "basic": {
                    "keywords": ["ç®€å•", "å¿«é€Ÿ", "ç›´æ¥", "ä»€ä¹ˆæ˜¯", "æ€ä¹ˆæ ·", "hello", "hi", "ä½ å¥½"],
                    "base_confidence": 0.85
                },
                "enhanced": {
                    "keywords": ["åˆ†æ", "è¯¦ç»†", "æ·±å…¥", "å…¨é¢", "æ¯”è¾ƒ", "è¯„ä¼°", "ç­–ç•¥", "å¤æ‚", "ä¸“ä¸š", "é«˜çº§"],
                    "base_confidence": 0.8
                },
                "rag": {
                    "keywords": ["æŸ¥æ‰¾", "æœç´¢", "æ–‡æ¡£", "èµ„æ–™", "æ•°æ®åº“", "çŸ¥è¯†åº“", "å†å²", "ä¹‹å‰", "è®°å½•", "æ¡£æ¡ˆ"],
                    "base_confidence": 0.9
                },
                "agent": {
                    "keywords": ["æ‰§è¡Œ", "è°ƒç”¨", "å·¥å…·", "è®¡ç®—", "è¿è¡Œ", "å¤„ç†", "æ“ä½œ", "è‡ªåŠ¨"],
                    "base_confidence": 0.85
                }
            })
            
            best_route = "basic"
            max_confidence = 0.6
            
            # æ¡ä»¶è·¯ç”±è¯„ä¼°
            for route_name, route_config in route_conditions.items():
                confidence = route_config["base_confidence"]
                
                # å…³é”®è¯åŒ¹é…
                keyword_matches = sum(1 for keyword in route_config["keywords"] if keyword in text_lower)
                if keyword_matches > 0:
                    confidence += min(0.1, keyword_matches * 0.03)
                
                # æ¡ä»¶å‡½æ•°è¯„ä¼°
                try:
                    if self._check_route_conditions(route_name, base_analysis):
                        confidence += 0.1
                except:
                    pass
                
                # é€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦çš„è·¯ç”±
                if confidence > max_confidence:
                    max_confidence = confidence
                    best_route = route_name
            
            return best_route, min(0.95, max_confidence)
    
        def _analyze_message_basics(self, message: str) -> Dict[str, Any]:
            """åŸºç¡€æ¶ˆæ¯åˆ†æ"""
            
            text_lower = message.lower()
            words = message.split()
            
            analysis = {
                "length": len(message),
                "word_count": len(words),
                "is_question": "?" in message,
                "has_code": False,
                "complexity": 1,
                "needs_context": False,
                "needs_tools": False,
                "language": "chinese" if any('\u4e00' <= c <= '\u9fff' for c in message) else "english",
            }
            
            # ä»£ç æ£€æµ‹
            code_patterns = ["```", "def ", "class ", "import ", "function(", "ä»£ç ", "å‡½æ•°"]
            analysis["has_code"] = any(pattern in text_lower for pattern in code_patterns)
            
            # é—®é¢˜ç±»å‹æ£€æµ‹  
            question_patterns = ["ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "å“ªé‡Œ", "when", "what", "how", "why", "where", "æ˜¯å¦", "èƒ½å¦"]
            analysis["is_question"] = analysis["is_question"] or any(q in text_lower for q in question_patterns)
            
            # å¤æ‚åº¦è¯„ä¼°
            if len(message) > 500:
                analysis["complexity"] = 3
            elif len(message) > 200:
                analysis["complexity"] = 2
                
            # ä¸Šä¸‹æ–‡éœ€æ±‚æ£€æµ‹
            context_keywords = ["å†å²", "ä¹‹å‰", "æ–‡æ¡£", "èµ„æ–™", "è®°å½•", "ä¸Šæ¬¡", "å‰é¢", "ä»¥å‰", "æŸ¥æ‰¾", "æœç´¢", "è®°ä½", "å›å¿†"]
            analysis["needs_context"] = any(keyword in text_lower for keyword in context_keywords)
            
            # å·¥å…·éœ€æ±‚æ£€æµ‹
            tool_keywords = ["è®¡ç®—", "æœç´¢", "æŸ¥è¯¢", "åˆ†æ", "æ‰§è¡Œ", "è°ƒç”¨", "è¿è¡Œ", "å¤„ç†", "ç”Ÿæˆ", "ç¿»è¯‘"]
            analysis["needs_tools"] = any(keyword in text_lower for keyword in tool_keywords)
            
            return analysis
    
        def _check_route_conditions(self, route_name: str, analysis: Dict[str, Any]) -> bool:
            """æ£€æŸ¥è·¯ç”±æ¡ä»¶"""
            if route_name == "basic":
                return analysis["complexity"] == 1 and not analysis["needs_tools"] and not analysis["needs_context"]
            elif route_name == "enhanced":
                return analysis["complexity"] >= 2 or analysis["has_code"]
            elif route_name == "rag":
                return analysis["needs_context"]
            elif route_name == "agent":
                return analysis["needs_tools"]
            return False
    
        def _validate_explicit_route(self, hint: str) -> str:
            """éªŒè¯æ˜¾å¼è·¯ç”±æç¤º"""
            valid_routes = ["basic", "enhanced", "rag", "agent", "complex"]
            hint_lower = hint.lower().strip()
            
            if hint_lower in valid_routes:
                return hint_lower
            
            # æ¨¡ç³ŠåŒ¹é…
            route_mapping = {
                "ç®€å•": "basic", "åŸºç¡€": "basic", "å¿«é€Ÿ": "basic",
                "å¢å¼º": "enhanced", "å¤æ‚": "enhanced", "é«˜çº§": "enhanced",
                "æ£€ç´¢": "rag", "æœç´¢": "rag", "æ–‡æ¡£": "rag",
                "ä»£ç†": "agent", "å·¥å…·": "agent", "æ‰§è¡Œ": "agent"
            }
            
            return route_mapping.get(hint_lower, "basic")
    
        def _determine_priority(self, route: str, confidence: float) -> str:
            """ç¡®å®šå¤„ç†ä¼˜å…ˆçº§"""
            priority_map = {
                "basic": "low",
                "enhanced": "high", 
                "rag": "medium",
                "agent": "high",
                "complex": "critical"
            }
            
            base_priority = priority_map.get(route, "normal")
            
            # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´ä¼˜å…ˆçº§
            if confidence < 0.7:
                return "normal"
            
            return base_priority
    
        def _estimate_tokens(self, length: int) -> int:
            """ä¼°ç®—tokenæ•°é‡"""
            # ç®€åŒ–çš„tokenä¼°ç®—
            return max(1, int(length / 4))
    
        def _create_base_message(self, message: str, env_config: Dict[str, Any]) -> Message:
            """åˆ›å»ºåŸºç¡€æ¶ˆæ¯å¯¹è±¡"""
            session_id = self.session_id or env_config.get("session_id", f"session_{uuid.uuid4().hex[:8]}")
            
            base_message = Message(
                text=message,
                sender=self.sender,
                sender_name=self.sender_name,
                session_id=session_id,
            )
            
            # å®‰å…¨çš„æ–‡ä»¶å¤„ç†
            if self.files:
                validated_files = []
                for file in self.files:
                    try:
                        if hasattr(file, 'name') and file.name:
                            validated_files.append(file)
                    except Exception:
                        continue
                base_message.files = validated_files
                
            return base_message
    
        def _create_langchainjs_output(self, message: Message, analysis: Dict[str, Any], 
                                      env_config: Dict[str, Any]) -> Data:
            """åˆ›å»ºLangChain.jså…¼å®¹çš„è¾“å‡ºæ ¼å¼"""
            
            # LangChain.jsæ ‡å‡†æ ¼å¼
            langchainjs_data = {
                # æ ‡å‡†Runnableæ¥å£
                "input": message.text,
                "metadata": analysis,  # æ‰å¹³åŒ–çš„å…ƒæ•°æ®
                
                # LangChain.jsç‰¹å®šå­—æ®µ
                "type": "chat_input",
                "source": "langflow",
                "version": "1.0",
                "timestamp": datetime.now().isoformat(),
                
                # è·¯ç”±ä¿¡æ¯
                "routing": {
                    "decision": analysis["routing_decision"],
                    "confidence": analysis["confidence"],
                    "mode": analysis["routing_mode"]
                },
                
                # æ¶ˆæ¯ä¿¡æ¯
                "message": {
                    "content": message.text,
                    "role": "user",
                    "session_id": message.session_id,
                    "sender": message.sender
                },
                
                # å¤„ç†é…ç½®
                "config": {
                    "model": env_config.get("model_configs", {}).get(analysis["routing_decision"], {}).get("model", self.model_preference),
                    "temperature": env_config.get("model_configs", {}).get(analysis["routing_decision"], {}).get("temperature", 0.3)
                }
            }
            
            self.status = f"ğŸ”— LangChain.js Ready: {analysis['routing_decision'].upper()} (conf: {analysis['confidence']:.2f})"
            
            return Data(
                data=langchainjs_data,
                text=message.text
            )
    
        def _create_lcel_output_format(self, message: Message, analysis: Dict[str, Any], 
                                      env_config: Dict[str, Any]) -> Data:
            """åˆ›å»ºLCELå…¼å®¹çš„è¾“å‡ºæ ¼å¼"""
            
            # LCELå…¼å®¹çš„æ•°æ®ç»“æ„
            lcel_data = {
                # æ ‡å‡†LCELè¾“å…¥æ ¼å¼
                "input": message.text,
                "message": message,
                
                # æ‰å¹³åŒ–çš„è·¯ç”±åˆ†æç»“æœ  
                **analysis,
                
                # ä¸ºä¸‹æ¸¸RunnableLambdaå‡†å¤‡çš„æ¡ä»¶æ•°æ®
                "conditions": {
                    "is_basic": analysis["routing_decision"] == "basic",
                    "is_enhanced": analysis["routing_decision"] == "enhanced", 
                    "is_rag": analysis["routing_decision"] == "rag",
                    "is_agent": analysis["routing_decision"] == "agent",
                    "needs_llm_analysis": analysis["requires_llm_routing"]
                },
                
                # ç¯å¢ƒé…ç½®ä¿¡æ¯
                "env_mode": "vercel" if self.vercel_mode else "standard"
            }
            
            self.status = f"âœ… LCEL Ready: {analysis['routing_decision'].upper()} (conf: {analysis['confidence']:.2f})"
            
            return Data(
                data=lcel_data,
                text=message.text
            )
    
        def _create_openai_output(self, message: Message, analysis: Dict[str, Any], 
                                 env_config: Dict[str, Any]) -> Data:
            """åˆ›å»ºOpenAIæ ¼å¼å…¼å®¹çš„è¾“å‡º"""
            
            model_config = env_config.get("model_configs", {}).get(analysis["routing_decision"], {})
            
            openai_data = {
                "messages": [{
                    "role": "user",
                    "content": message.text,
                    "metadata": analysis if self.flatten_metadata else {"routing_hint": analysis["routing_decision"]}
                }],
                "model": model_config.get("model", self.model_preference),
                "temperature": model_config.get("temperature", 0.3),
                "routing_context": analysis,
                "stream": False
            }
            
            self.status = f"ğŸ”„ OpenAI Format: {analysis['routing_decision']} ready"
            
            return Data(data=openai_data)
    
        def _create_standard_output(self, message: Message, analysis: Dict[str, Any], 
                                   env_config: Dict[str, Any]) -> Data:
            """åˆ›å»ºæ ‡å‡†LangFlowè¾“å‡ºæ ¼å¼"""
            return Data(
                data={
                    "type": "enhanced_chat_input",
                    "message": message,
                    "routing_analysis": analysis,
                    "timestamp": datetime.now().isoformat(),
                    "env_config": env_config if not env_config.get("exclude_env_from_output") else {}
                }
            )
    
        def _create_error_output(self, error_msg: str, env_config: Dict[str, Any]) -> Data:
            """åˆ›å»ºé”™è¯¯è¾“å‡º"""
            error_message = Message(
                text=f"è¾“å…¥å¤„ç†é”™è¯¯: {error_msg}",
                sender="system"
            )
            
            # é”™è¯¯æ—¶çš„åŸºç¡€è·¯ç”±åˆ†æ
            basic_analysis = {
                "routing_decision": "basic",
                "confidence": 0.5,
                "requires_llm_routing": False,
                "error": True,
                "error_message": error_msg
            }
            
            return Data(
                data={
                    "type": "error",
                    "message": error_message,
                    "routing_analysis": basic_analysis,
                    "timestamp": datetime.now().isoformat()
                }
            )
        # åœ¨ä½ çš„ç»„ä»¶ä¸­æ·»åŠ memoryå¤„ç†æ–¹æ³•
        def format_memory_for_export(self, memory_message):
            """æ ¼å¼åŒ–memoryä¸ºå¯¼å‡ºå‹å¥½çš„æ ¼å¼"""
            if not memory_message:
                return None
    
            # æå–ChatMemoryå†…å®¹
            memory_content = memory_message.text if hasattr(memory_message, 'text') else str(memory_message)
    
            # è§£æå¯¹è¯å†å²
            conversations = []
            lines = memory_content.split('\n')
            for line in lines:
                if line.startswith('Human:') or line.startswith('User:'):
                    conversations.append({"role": "user", "content": line.split(':', 1)[1].strip()})
                elif line.startswith('Assistant:') or line.startswith('AI:'):
                    conversations.append({"role": "assistant", "content": line.split(':', 1)[1].strip()})
    
            return {
                "type": "chat_memory",
                "messages": conversations[-10:],  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
                "format": "langchain_compatible"
            }
        # ç»Ÿä¸€çš„memory metadataæ ¼å¼
        def create_memory_metadata(self, memory_data):
            return {
                "memory": {
                    "has_memory": bool(memory_data),
                    "format": "langflow_chat_memory",
                    "messages": memory_data.get("messages", []) if memory_data else [],
                    "export_ready": True
                },
                "langchain_export": {
                    "memory_compatible": True,
                    "vercel_ready": True
                }
            }
    