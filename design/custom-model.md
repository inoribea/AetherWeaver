    from __future__ import annotations
    
    import time
    import os
    import json
    from typing import Any, Dict, List, Union, Optional
    from datetime import datetime
    
    from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough
    from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
    
    from langflow.custom.custom_component.component import Component
    from langflow.schema.dotdict import dotdict
    from langflow.field_typing.range_spec import RangeSpec  
    from langflow.io import (
        HandleInput,
        MessageInput,
        Output,
        SecretStrInput,
        DropdownInput,
        BoolInput,
        SliderInput, 
        MultilineInput,
        StrInput,
    )
    from langflow.schema.message import Message
    
    class CustomLanguageModelComponent(Component, Runnable):
        display_name = "Main Router Model (LCEL/LangChain.js Compatible)"
        description = "LCEL-compatible language model with customizable API endpoints, routing optimization, RAG support, Agent tools and LangChain.js export"
        icon = "cpu"
        name = "CustomLanguageModelComponent"
    
        inputs = [
            DropdownInput(
                name="provider",
                display_name="Model Provider",
                options=["OpenAI", "Anthropic", "Google", "DeepSeek", "OpenAI-Compatible"],
                value="OpenAI",
                info="Select the model provider",
                options_metadata=[{"icon": "OpenAI"}, {"icon": "Anthropic"}, {"icon": "GoogleGenerativeAI"},{"icon": "DeepSeek"},  {"icon": "Settings"}],
                real_time_refresh=True,
            ),
            BoolInput(
                name="use_custom_model",
                display_name="Use Custom Model",
                value=False,
                info="Enable to use custom model name instead of predefined list",
                real_time_refresh=True,
                advanced=False,
            ),
            StrInput(
                name="custom_model_name",
                display_name="Custom Model Name",
                info="Enter custom model name",
                show=False,
                advanced=False,
                real_time_refresh=True,
                placeholder="Input your custom model name.",
            ),
            DropdownInput(
                name="model_name",
                display_name="Model Name",
                options=["gpt-image-1", "gpt-5", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano","o4-mini","o3-pro-all","text-embedding-3-small",  "claude-sonnet-4-all", "claude-opus-4-thinking-all", "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite-preview-06-17", "deepseek-chat", "deepseek-reasoner"],
                value="gpt-5",
                info="Select the model to use",
                real_time_refresh=True,
                show=True,
            ),
            # æ–°å¢å…¼å®¹æ€§å¼€å…³
            BoolInput(
                name="vercel_mode",
                display_name="Vercel Environment Mode",
                value=False,
                info="Enable Vercel environment variable support for deployment",
                advanced=True,
            ),
            BoolInput(
                name="langchainjs_export",
                display_name="LangChain.js Export Mode",
                value=False,
                info="Enable LangChain.js compatible configuration and export",
                advanced=True,
            ),
            BoolInput(
                name="flatten_metadata",
                display_name="Flatten Metadata",
                value=True,
                info="Use flattened metadata structure for LCEL compatibility",
                advanced=True,
            ),
            BoolInput(
                name="enable_runnable_interface",
                display_name="Enable Runnable Interface",
                value=True,
                info="Make component fully LCEL Runnable compatible",
                advanced=True,
            ),
            BoolInput(
                name="enable_langserve_mode",
                display_name="Enable LangServe Mode",
                value=False,
                info="Enable LangServe deployment optimization",
                advanced=True,
            ),
            SecretStrInput(
                name="openai_api_key",
                display_name="OpenAI API Key",
                info="Your OpenAI API key (supports Vercel env vars)",
                value="",
                advanced=True,
            ),
            SecretStrInput(
                name="anthropic_api_key", 
                display_name="Anthropic API Key",
                info="Your Anthropic API key (supports Vercel env vars)",
                value="",
                advanced=True,
            ),
            SecretStrInput(
                name="google_api_key", 
                display_name="Google API Key",
                info="Your Google API key (supports Vercel env vars)",
                value="",
                advanced=True,
            ),
            BoolInput(
                name="use_custom_url",
                display_name="Use Custom Base URL",
                value=False,
                info="Enable to use custom API endpoint URL",
                advanced=True,
            ),
            StrInput(
                name="base_url",
                display_name="Custom Base URL",
                info="Custom API endpoint URL",
                placeholder="https://api.example.com/v1",
                show=True,
                advanced=True,
            ),
            MessageInput(
                name="input_value",
                display_name="Input",
                info="User input message or prompt from AgentPromptComponent",
                required=True,
            ),
            MessageInput(
                name="routing_context",
                display_name="Routing Context",
                info="Routing information from SmartRouting component",
                required=False,
            ),
            MessageInput(
                name="context_documents",
                display_name="Context Documents",
                info="Retrieved documents from vector store for RAG",
                required=False,
            ),
            MessageInput(
                name="tools",
                display_name="Tools",
                info="Available tools for agent execution (Tavily Search, ArXiv, etc.)",
                required=False,
            ),
            MessageInput(
                name="memory_context",
                display_name="Memory Context",
                info="Memory context from ChatMemory component",
                required=False,
            ),
            MultilineInput(
                name="system_message",
                display_name="System Message", 
                info="Base system message (will be enhanced with memory and routing)",
                value="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ã€‚",
                show=False,
                advanced=True,
            ),
            BoolInput(
                name="stream_output",
                display_name="Stream",
                info="Enable streaming responses",
                value=False,
            ),
            SliderInput(
                name="temperature",
                display_name="Temperature",
                info="Controls randomness (0.0 to 2.0)",
                value=0.1,
                range_spec=RangeSpec(min=0.0, max=2.0, step=0.01, step_type="float"),
                advanced=True,
            ),
            SliderInput(
                name="max_tokens",
                display_name="Max Tokens",
                info="Maximum number of tokens to generate",
                value=1000,
                range_spec=RangeSpec(min=1, max=8192, step=1, step_type="int"),
                advanced=True,
            ),
            SliderInput(
                name="top_p",
                display_name="Top P",
                info="Controls diversity via nucleus sampling (0.0 to 1.0)",
                value=1.0,
                range_spec=RangeSpec(min=0.01, max=1.0, step=0.01, step_type="float"),
                advanced=True,
            ),
        ]
    
        outputs = [
            Output(
                display_name="LCEL Model Output",
                name="lcel_model_output",
                method="get_lcel_model_output",
            ),
        ]
    
        def _load_environment_config(self) -> Dict[str, Any]:
            """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒVercelæ¨¡å¼"""
            
            default_config = {
                "model_configurations": {
                    "openai": {
                        "api_key_env": "OPENAI_API_KEY",
                        "default_models": ["gpt-image-1", "gpt-5", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano","o4-mini","o3-pro-all","text-embedding-3-small"],
                        "supports_json_mode": True,
                        "supports_tools": True
                    },
                    "anthropic": {
                        "api_key_env": "ANTHROPIC_API_KEY",
                        "default_models": ["claude-sonnet-4-all", "claude-opus-4-thinking-all", "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite-preview-06-17"],
                        "supports_json_mode": False,
                        "supports_tools": True
                    },
                    "google": {
                        "api_key_env": "GOOGLE_API_KEY",
                        "default_models": ["gemini-2.5-pro", "gemini-2.5-flash"],
                        "supports_json_mode": True,
                        "supports_tools": True
                    },
                    "deepseek": {
                        "api_key_env": "DEEPSEEK_API_KEY",
                        "default_models": ["deepseek-chat", "deepseek-reasoner"],
                        "supports_json_mode": True,
                        "supports_tools": True
                    }
                },
                "routing_enhancements": {
                    "basic": {"temperature_adjust": -0.1, "max_tokens_adjust": -200},
                    "enhanced": {"temperature_adjust": 0.1, "max_tokens_adjust": 500},
                    "rag": {"temperature_adjust": -0.2, "max_tokens_adjust": 0},
                    "agent": {"temperature_adjust": 0.0, "max_tokens_adjust": 200}
                },
                "memory_integration": {
                    "langflow_chat_memory": True,
                    "custom_memory": True,
                    "max_history_tokens": 2000,
                    "summarize_long_history": True
                },
                "lcel_configuration": {
                    "enable_runnable": self.enable_runnable_interface,
                    "enable_streaming": True,
                    "enable_async": True,
                    "enable_batching": True,
                    "langserve_mode": self.enable_langserve_mode
                },
                "langchainjs_compatibility": {
                    "export_format": self.langchainjs_export,
                    "flatten_metadata": self.flatten_metadata,
                    "runnable_interface": self.enable_runnable_interface,
                    "model_field_name": "model",  # ä½¿ç”¨"model"è€Œä¸æ˜¯"model_name"
                    "openai_format": True
                }
            }
            
            try:
                if self.vercel_mode:
                    config_str = os.getenv("LANGFLOW_LANGUAGE_MODEL_CONFIG", "{}")
                    vercel_config = json.loads(config_str) if config_str else {}
                    default_config.update(vercel_config)
                    
                    # Vercelç‰¹å®šé…ç½®
                    default_config["deployment"] = {
                        "env": "vercel",
                        "edge_compatible": True,
                        "region": os.getenv("VERCEL_REGION", "unknown"),
                        "function_timeout": int(os.getenv("VERCEL_FUNCTION_TIMEOUT", "30"))
                    }
                else:
                    config_str = os.getenv("LANGUAGE_MODEL_CONFIG", "{}")
                    env_config = json.loads(config_str) if config_str else {}
                    default_config.update(env_config)
                    
            except (json.JSONDecodeError, TypeError):
                pass
                
            return default_config
    
        def extract_routing_info(self, routing_context: Message = None) -> dict:
            """å®‰å…¨æå–è·¯ç”±ä¿¡æ¯ï¼ŒåŒ…æ‹¬è®°å¿†"""
            routing_info = {
                "route": "auto",
                "confidence": 0.7,
                "language": "auto",
                "is_chinese": False,
                "conversation_history": "",
                "has_memory": False,
                "metadata": {},
                "analysis_method": "unknown",
                "memory_format": "none",
                "original_text": ""
            }
            
            try:
                if routing_context and hasattr(routing_context, 'metadata') and routing_context.metadata:
                    metadata = routing_context.metadata
                    routing_info["metadata"] = metadata
                    
                    # æ”¯æŒæ‰å¹³åŒ–å’ŒåµŒå¥—æ ¼å¼
                    if self.flatten_metadata:
                        # æ‰å¹³åŒ–æ ¼å¼
                        routing_info.update({
                            "route": metadata.get('routing_decision') or metadata.get('route', 'auto'),
                            "confidence": metadata.get('confidence', 0.7),
                            "conversation_history": metadata.get('conversation_history', ''),
                            "has_memory": metadata.get('has_memory', False),
                            "analysis_method": metadata.get('analysis_method', 'unknown'),
                            "memory_format": metadata.get('memory_format', 'none'),
                            "original_text": metadata.get('original_user_input', ''),
                            "is_chinese": metadata.get('routing_is_chinese', False)
                        })
                    else:
                        # åµŒå¥—æ ¼å¼
                        memory_info = metadata.get('memory', {})
                        routing_info.update({
                            "route": metadata.get('routing_decision') or metadata.get('route', 'auto'),
                            "confidence": metadata.get('confidence', 0.7),
                            "conversation_history": memory_info.get('conversation_history', ''),
                            "has_memory": memory_info.get('has_memory', False),
                            "analysis_method": metadata.get('analysis_method', 'unknown'),
                            "memory_format": memory_info.get('memory_format', 'none'),
                            "original_text": metadata.get('original_user_input', ''),
                            "is_chinese": metadata.get('routing_is_chinese', False)
                        })
                            
            except Exception as e:
                self.status = f"âš ï¸ è·¯ç”±ä¿¡æ¯æå–å¤±è´¥: {str(e)[:30]}..."
            
            return routing_info
    
        def extract_context_documents(self, context_documents: Message = None) -> List[str]:
            """æå–å¹¶æ ¼å¼åŒ–ä¸Šä¸‹æ–‡æ–‡æ¡£ä¸ºåˆ—è¡¨"""
            if not context_documents:
                return []
            
            try:
                docs = []
                
                if hasattr(context_documents, 'text') and context_documents.text:
                    text = str(context_documents.text)
                    if '\n\n' in text:
                        docs = [doc.strip() for doc in text.split('\n\n') if doc.strip()]
                    else:
                        docs = [text]
                        
                elif hasattr(context_documents, 'data') and context_documents.data:
                    data = context_documents.data
                    if isinstance(data, list):
                        for doc in data:
                            if hasattr(doc, 'page_content'):
                                docs.append(doc.page_content)
                            elif hasattr(doc, 'text'):
                                docs.append(doc.text)
                            elif isinstance(doc, str):
                                docs.append(doc)
                            else:
                                docs.append(str(doc))
                    else:
                        if hasattr(data, 'page_content'):
                            docs.append(data.page_content)
                        elif hasattr(data, 'text'):
                            docs.append(data.text)
                        else:
                            docs.append(str(data))
                else:
                    docs = [str(context_documents)]
                
                cleaned_docs = []
                for doc in docs:
                    cleaned = doc.strip()
                    if cleaned and len(cleaned) > 10:
                        cleaned_docs.append(cleaned)
                
                return cleaned_docs
                    
            except Exception as e:
                self.status = f"âš ï¸ æ–‡æ¡£æå–å¤±è´¥: {str(e)[:30]}..."
                return []
    
        def extract_tools_info(self, tools: Message = None) -> List[Dict[str, Any]]:
            """æå–å¹¶æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯ä¸ºåˆ—è¡¨"""
            if not tools:
                return []
            
            try:
                tools_list = []
                
                if hasattr(tools, 'text') and tools.text:
                    text = str(tools.text)
                    tools_list.append({"name": "text_tool", "description": text})
                    
                elif hasattr(tools, 'data') and tools.data:
                    data = tools.data
                    if isinstance(data, list):
                        for tool in data:
                            if isinstance(tool, dict):
                                tools_list.append(tool)
                            else:
                                tools_list.append({"name": "unknown_tool", "description": str(tool)})
                    else:
                        if isinstance(data, dict):
                            tools_list.append(data)
                        else:
                            tools_list.append({"name": "unknown_tool", "description": str(data)})
                else:
                    tools_list.append({"name": "unknown_tool", "description": str(tools)})
                
                return tools_list
                    
            except Exception as e:
                self.status = f"âš ï¸ å·¥å…·æå–å¤±è´¥: {str(e)[:30]}..."
                return []
    
        def extract_memory_context(self, memory_context: Message = None) -> Dict[str, Any]:
            """æå–è®°å¿†ä¸Šä¸‹æ–‡ä¿¡æ¯"""
            memory_info = {
                "has_memory": False,
                "content": "",
                "format": "none",
                "processed_content": ""
            }
            
            try:
                if not memory_context:
                    return memory_info
                
                if hasattr(memory_context, 'text') and memory_context.text:
                    content = str(memory_context.text)
                    memory_info.update({
                        "has_memory": True,
                        "content": content,
                        "format": "text",
                        "processed_content": content
                    })
                
                if hasattr(memory_context, 'metadata') and memory_context.metadata:
                    metadata = memory_context.metadata
                    if self.flatten_metadata:
                        memory_info.update({
                            "format": metadata.get('memory_format', 'text'),
                            "conversation_rounds": metadata.get('memory_conversation_rounds', 0)
                        })
                    else:
                        memory_meta = metadata.get('memory_info', {})
                        memory_info.update({
                            "format": memory_meta.get('format', 'text'),
                            "conversation_rounds": memory_meta.get('conversation_rounds', 0)
                        })
            
            except Exception as e:
                self.status = f"âš ï¸ è®°å¿†æå–å¤±è´¥: {str(e)[:30]}..."
            
            return memory_info
    
        def _detect_chinese(self, text: str) -> bool:
            """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡"""
            if not text:
                return False
            
            chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
            return chinese_chars > len(text) * 0.3
    
        def get_optimized_system_message(self, routing_context: Message = None, 
                                        memory_context: Message = None) -> str:
            """æ ¹æ®è·¯ç”±ä¸Šä¸‹æ–‡å’Œè®°å¿†åŠ¨æ€è·å–system message"""
            
            routing_info = self.extract_routing_info(routing_context)
            memory_info = self.extract_memory_context(memory_context)
            routing_mode = routing_info["route"]
            is_chinese = routing_info["is_chinese"]
            conversation_history = routing_info["conversation_history"]
            has_memory = routing_info["has_memory"] or memory_info["has_memory"]
            
            base_system_message = self.system_message
            
            if is_chinese:
                route_prompts = {
                    "basic": f"{base_system_message}\n\nå½“å‰æ¨¡å¼ï¼šåŸºç¡€å¯¹è¯æ¨¡å¼ã€‚è¯·æä¾›ç®€æ´ã€ç›´æ¥ã€å‹å¥½çš„å›ç­”ã€‚",
                    "enhanced": f"{base_system_message}\n\nå½“å‰æ¨¡å¼ï¼šå¢å¼ºåˆ†ææ¨¡å¼ã€‚è¯·æä¾›æ·±å…¥ã€ä¸“ä¸šã€è¯¦ç»†çš„åˆ†æå’Œè§£ç­”ã€‚",
                    "rag": f"{base_system_message}\n\nå½“å‰æ¨¡å¼ï¼šçŸ¥è¯†æ£€ç´¢æ¨¡å¼ã€‚åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œç¡®ä¿ç­”æ¡ˆå‡†ç¡®æ€§ã€‚",
                    "agent": f"{base_system_message}\n\nå½“å‰æ¨¡å¼ï¼šå·¥å…·æ‰§è¡Œæ¨¡å¼ã€‚ä½ å¯ä»¥ä½¿ç”¨æä¾›çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œå¹¶åŸºäºå·¥å…·ç»“æœæä¾›ç­”æ¡ˆã€‚"
                }
            else:
                route_prompts = {
                    "basic": f"{base_system_message}\n\nMode: Basic conversation. Provide concise, direct answers.",
                    "enhanced": f"{base_system_message}\n\nMode: Enhanced analysis. Provide in-depth, professional analysis.",
                    "rag": f"{base_system_message}\n\nMode: Knowledge retrieval. Answer based on provided reference documents.",
                    "agent": f"{base_system_message}\n\nMode: Tool execution. You can use the provided tools to complete tasks. Choose appropriate tools based on user needs and provide answers based on tool results."
                }
            
            dynamic_prompt = route_prompts.get(routing_mode, route_prompts["basic"])
            
            # æ•´åˆè®°å¿†ä¿¡æ¯
            all_memory_content = ""
            if conversation_history:
                all_memory_content = conversation_history
            elif memory_info["has_memory"]:
                all_memory_content = memory_info["processed_content"]
            
            if has_memory and all_memory_content:
                memory_section = f"""
    
    ğŸ“ å¯¹è¯å†å²è®°å¿†:
    {all_memory_content}
    
    è¯·åŸºäºå¯¹è¯å†å²æä¾›è¿è´¯çš„å›ç­”ã€‚""" if is_chinese else f"""
    
    ğŸ“ Conversation History:
    {all_memory_content}
    
    Please provide coherent responses based on conversation history."""
                
                dynamic_prompt += memory_section
            
            return dynamic_prompt
    
        def get_effective_model_name(self) -> str:
            """è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°"""
            if self.use_custom_model and hasattr(self, 'custom_model_name') and self.custom_model_name.strip():
                return self.custom_model_name.strip()
            else:
                return self.model_name
    
        def get_effective_api_key(self, provider: str, env_config: Dict[str, Any]) -> str:
            """è·å–æœ‰æ•ˆçš„APIå¯†é’¥ï¼Œæ”¯æŒVercelæ¨¡å¼"""
            
            # é¦–å…ˆå°è¯•ä»ç»„ä»¶è¾“å…¥è·å–
            if provider == "OpenAI":
                api_key = self.openai_api_key
            elif provider == "Anthropic":
                api_key = self.anthropic_api_key
            elif provider == "Google":
                api_key = self.google_api_key
            else:
                api_key = ""
            
            # å¦‚æœæ²¡æœ‰æˆ–è€…æ˜¯Vercelæ¨¡å¼ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            if not api_key or self.vercel_mode:
                model_config = env_config.get("model_configurations", {}).get(provider.lower(), {})
                env_key = model_config.get("api_key_env", f"{provider.upper()}_API_KEY")
                env_api_key = os.getenv(env_key)
                if env_api_key:
                    api_key = env_api_key
            
            return api_key
    
        def adjust_parameters_for_route(self, routing_info: Dict[str, Any], 
                                       env_config: Dict[str, Any]) -> Dict[str, Any]:
            """æ ¹æ®è·¯ç”±è°ƒæ•´æ¨¡å‹å‚æ•°"""
            
            route = routing_info["route"]
            routing_enhancements = env_config.get("routing_enhancements", {})
            route_config = routing_enhancements.get(route, {})
            
            # åŸºç¡€å‚æ•°
            adjusted_params = {
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "top_p": self.top_p
            }
            
            # æ ¹æ®è·¯ç”±è°ƒæ•´å‚æ•°
            temp_adjust = route_config.get("temperature_adjust", 0)
            token_adjust = route_config.get("max_tokens_adjust", 0)
            
            adjusted_params["temperature"] = max(0.0, min(2.0, adjusted_params["temperature"] + temp_adjust))
            adjusted_params["max_tokens"] = max(1, min(8192, adjusted_params["max_tokens"] + token_adjust))
            
            return adjusted_params
    
        def build_messages(self, routing_context: Message = None, 
                          memory_context: Message = None) -> list:
            """æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ”¯æŒ RAG å’Œ Agent æ¨¡å¼ï¼Œå…¼å®¹LCEL"""
            
            routing_info = self.extract_routing_info(routing_context)
            route = routing_info["route"]
            is_chinese = routing_info["is_chinese"]
            
            user_input = self.input_value.text if hasattr(self.input_value, 'text') else str(self.input_value)
            
            optimized_system_message = self.get_optimized_system_message(routing_context, memory_context)
            
            if route == "rag":
                context_docs = self.extract_context_documents(getattr(self, 'context_documents', None))
                
                if context_docs:
                    if is_chinese:
                        docs_text = ""
                        for i, doc in enumerate(context_docs, 1):
                            docs_text += f"æ–‡æ¡£{i}:\n{doc}\n\n"
                        
                        final_user_message = f"""å‚è€ƒä»¥ä¸‹æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜:
    
    {docs_text}ç”¨æˆ·é—®é¢˜: {user_input}"""
                    else:
                        docs_text = ""
                        for i, doc in enumerate(context_docs, 1):
                            docs_text += f"Document {i}:\n{doc}\n\n"
                        
                        final_user_message = f"""Please answer the user's question based on the following reference documents:
    
    {docs_text}User Question: {user_input}"""
                else:
                    if is_chinese:
                        final_user_message = f"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å‚è€ƒæ–‡æ¡£ã€‚è¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜:\n\n{user_input}"
                    else:
                        final_user_message = f"No relevant reference documents found. Please answer the following question based on your knowledge:\n\n{user_input}"
            
            elif route == "agent":
                tools_info = self.extract_tools_info(getattr(self, 'tools', None))
                
                if tools_info:
                    if is_chinese:
                        tools_text = "å¯ç”¨å·¥å…·:\n"
                        for i, tool in enumerate(tools_info, 1):
                            tool_name = tool.get('name', f'å·¥å…·{i}')
                            tool_desc = tool.get('description', 'æ— æè¿°')
                            tools_text += f"{i}. {tool_name}: {tool_desc}\n"
                        
                        final_user_message = f"""{tools_text}
    
    ç”¨æˆ·è¯·æ±‚: {user_input}
    
    è¯·é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆç”¨æˆ·è¯·æ±‚ã€‚"""
                    else:
                        tools_text = "Available tools:\n"
                        for i, tool in enumerate(tools_info, 1):
                            tool_name = tool.get('name', f'Tool{i}')
                            tool_desc = tool.get('description', 'No description')
                            tools_text += f"{i}. {tool_name}: {tool_desc}\n"
                        
                        final_user_message = f"""{tools_text}
    
    User Request: {user_input}
    
    Please choose appropriate tools to fulfill the user's request."""
                else:
                    if is_chinese:
                        final_user_message = f"å½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ã€‚è¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜:\n\n{user_input}"
                    else:
                        final_user_message = f"No tools available. Please answer the following question based on your knowledge:\n\n{user_input}"
            
            else:
                final_user_message = user_input
            
            effective_model_name = self.get_effective_model_name()
            is_o1_model = effective_model_name and effective_model_name.startswith("o1")
            
            # LCELå…¼å®¹çš„æ¶ˆæ¯æ„å»º
            if optimized_system_message and not is_o1_model:
                messages = [
                    SystemMessage(content=optimized_system_message),
                    HumanMessage(content=final_user_message)
                ]
                return messages
            else:
                if is_o1_model and optimized_system_message:
                    combined_message = f"{optimized_system_message}\n\n{final_user_message}"
                else:
                    combined_message = final_user_message
                
                return [HumanMessage(content=combined_message)]
    
        def build_model(self, env_config: Dict[str, Any], routing_info: Dict[str, Any]):
            """æ„å»ºè¯­è¨€æ¨¡å‹å®ä¾‹ï¼Œå…¼å®¹LCELå’ŒLangChain.jsï¼Œå¹¶æ­£ç¡®å¤„ç†è‡ªå®šä¹‰URL"""
            effective_model_name = self.get_effective_model_name()
            
            if self.use_custom_model and not hasattr(self, 'custom_model_name'):
                raise ValueError("Custom model name is required when 'Use Custom Model' is enabled")
            
            # è·å–è°ƒæ•´åçš„å‚æ•°
            adjusted_params = self.adjust_parameters_for_route(routing_info, env_config)
            
            # ç¡®å®šå®é™…çš„æ¨¡å‹æä¾›å•† (å¦‚æœä½¿ç”¨è‡ªå®šä¹‰URLï¼Œåˆ™å¼ºåˆ¶ä¸ºOpenAI-Compatible)
            actual_provider = self.provider
            if self.use_custom_url and self.base_url and self.base_url.strip():
                # å½“ä½¿ç”¨è‡ªå®šä¹‰URLæ—¶ï¼Œå‡å®šOneAPIæä¾›çš„æ˜¯OpenAIå…¼å®¹æ¥å£
                actual_provider = "OpenAI-Compatible" 
     
            try:
                # ç»Ÿä¸€ä½¿ç”¨ ChatOpenAI æ¥å¤„ç†è‡ªå®šä¹‰ URL çš„æƒ…å†µ
                if actual_provider in ["OpenAI", "Deepseek", "OpenAI-Compatible", "Anthropic", "Google"]: # å°†Anthropicå’ŒGoogleä¹Ÿçº³å…¥OpenAIå…¼å®¹å¤„ç†
                    from langchain_openai import ChatOpenAI
                    # å¯¹äºAnthropicå’ŒGoogleï¼Œå¦‚æœé€šè¿‡OneAPIä»£ç†ï¼Œå…¶API Keyå®é™…å°±æ˜¯OpenAI-Compatibleçš„Key
                    # æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œéœ€è¦æ ¹æ®åŸå§‹provideræ¥è·å–å¯¹åº”çš„keyï¼ŒOneAPIä¼šåšè½¬å‘
                    # æ³¨æ„ï¼šOneAPIå¯èƒ½åªæ¥å—OpenAIæ ¼å¼çš„API Keyï¼Œä½ éœ€è¦ç¡®ä¿ä¼ å…¥çš„Keyæ ¼å¼æ­£ç¡®
                    if self.provider == "Anthropic":
                        api_key = self.get_effective_api_key("Anthropic", env_config) # å°½ç®¡æ˜¯OpenAIå®¢æˆ·ç«¯ï¼Œä½†ä½¿ç”¨Anthropicçš„keyå˜é‡å
                    elif self.provider == "Google":
                        api_key = self.get_effective_api_key("Google", env_config) # åŒä¸Š
                    elif self.provider == "Deepseek":
                        api_key = self.get_effective_api_key("Deepseek", env_config)
                    else: # OpenAI å’Œ OpenAI-Compatible
                        api_key = self.get_effective_api_key("OpenAI", env_config)
                    
                    if not api_key:
                        raise ValueError(f"{self.provider} API key is required, or Custom Base URL is not correctly configured for OpenAI-Compatible mode.")
                    
                    # ç¡®å®š base_url
                    # å¦‚æœæ˜¯OpenAI-Compatibleæˆ–å¼€å¯äº†use_custom_urlï¼Œåˆ™ä½¿ç”¨è‡ªå®šä¹‰url
                    # å¦åˆ™ï¼Œå¯¹äºOpenAIï¼Œé»˜è®¤Langchainä¼šä½¿ç”¨å…¶å®˜æ–¹URL
                    base_url_to_use = self.base_url if self.use_custom_url or actual_provider == "OpenAI-Compatible" else None
                    
                    openai_kwargs = {
                        "model": effective_model_name,
                        "api_key": api_key,
                        "temperature": adjusted_params["temperature"],
                        "max_tokens": adjusted_params["max_tokens"],
                        "top_p": adjusted_params["top_p"],
                        "streaming": self.stream_output,
                    }
                    
                    if base_url_to_use:
                        openai_kwargs["base_url"] = base_url_to_use
                    
                    if self.enable_langserve_mode:
                        openai_kwargs["model_kwargs"] = {
                            "response_format": {"type": "text"}
                        }
                    
                    return ChatOpenAI(**openai_kwargs)
                    
                # å¦‚æœæ˜¯Anthropicå’ŒGoogleï¼Œä¸”æ²¡æœ‰ä½¿ç”¨è‡ªå®šä¹‰URLï¼Œåˆ™ç»§ç»­ä½¿ç”¨å„è‡ªçš„å®¢æˆ·ç«¯
                # è¿™ç§æƒ…å†µæ˜¯ä¸ºäº†ç›´æ¥è¿æ¥å®˜æ–¹APIï¼Œè€Œä¸æ˜¯é€šè¿‡OneAPI
                elif self.provider == "Anthropic":
                    from langchain_anthropic import ChatAnthropic
                    api_key = self.get_effective_api_key("Anthropic", env_config)
                    if not api_key:
                        raise ValueError("Anthropic API key is required")
                    
                    # Anthropic å®¢æˆ·ç«¯ä¸æ”¯æŒç›´æ¥çš„ base_url å‚æ•°ï¼Œé€šå¸¸é€šè¿‡ ANTHROPIC_API_BASE ç¯å¢ƒå˜é‡æ§åˆ¶
                    # å¦‚æœä½ åœ¨è¿™é‡Œä»ç„¶æƒ³é€šè¿‡ OneAPI ä»£ç†ï¼Œä½†åˆä¸æƒ³åˆ‡æ¢åˆ° OpenAI-Compatibleï¼Œ
                    # é‚£ä¹ˆä½ éœ€è¦åœ¨è¿™é‡Œè®¾ç½® os.environ["ANTHROPIC_API_BASE"] = self.base_url
                    # ä½†è¿™ç§åšæ³•ä¸æ¨èï¼Œå› ä¸ºç¯å¢ƒå˜é‡æ˜¯å…¨å±€çš„ï¼Œå¯èƒ½å½±å“å…¶ä»–ç»„ä»¶æˆ–æ¨¡å—ã€‚
                    # æ›´å¥½çš„åšæ³•æ˜¯ç»Ÿä¸€ä½¿ç”¨ ChatOpenAIã€‚
                    return ChatAnthropic(
                        model=effective_model_name,
                        api_key=api_key,
                        temperature=adjusted_params["temperature"],
                        max_tokens=adjusted_params["max_tokens"],
                        top_p=adjusted_params["top_p"],
                        streaming=self.stream_output,
                    )
                    
                elif self.provider == "Google":
                    from langchain_google_genai import ChatGoogleGenerativeAI
                    api_key = self.get_effective_api_key("Google", env_config)
                    if not api_key:
                        raise ValueError("Google API key is required")
                    
                    # Google GenAI å®¢æˆ·ç«¯ä¹Ÿä¸æ”¯æŒç›´æ¥çš„ base_url å‚æ•°
                    # åŒæ ·ï¼Œå¦‚æœè¦é€šè¿‡ OneAPI ä»£ç†ï¼Œæ¨èä½¿ç”¨ ChatOpenAI
                    return ChatGoogleGenerativeAI(
                        model=effective_model_name,
                        google_api_key=api_key,
                        temperature=adjusted_params["temperature"],
                        max_output_tokens=adjusted_params["max_tokens"],
                        top_p=adjusted_params["top_p"],
                        streaming=self.stream_output,
                    )
                
                else:
                    raise ValueError(f"Unsupported model provider: {self.provider}")
                    
            except Exception as e:
                raise ValueError(f"Failed to build model for {self.provider} with model {effective_model_name}: {str(e)}")
    
        # LCEL Runnableæ¥å£å®ç°
        def invoke(self, input_data: Any, config: Optional[Dict] = None) -> Message:
            """LCEL Runnable.invokeæ–¹æ³•å®ç°"""
            # ä¸´æ—¶è®¾ç½®è¾“å…¥æ•°æ®
            original_input = getattr(self, 'input_value', None)
            self.input_value = input_data if isinstance(input_data, Message) else Message(text=str(input_data))
            
            try:
                result = self.get_lcel_model_output()
                return result
            finally:
                # æ¢å¤åŸå§‹è¾“å…¥
                self.input_value = original_input
    
        async def ainvoke(self, input_data: Any, config: Optional[Dict] = None) -> Message:
            """LCEL Runnable.ainvokeå¼‚æ­¥æ–¹æ³•å®ç°"""
            return self.invoke(input_data, config)
    
        def stream(self, input_data: Any, config: Optional[Dict] = None):
            """LCEL Runnable.streamæ–¹æ³•å®ç°"""
            result = self.invoke(input_data, config)
            yield result
    
        async def astream(self, input_data: Any, config: Optional[Dict] = None):
            """LCEL Runnable.astreamå¼‚æ­¥æµæ–¹æ³•å®ç°"""
            result = await self.ainvoke(input_data, config)
            yield result
    
        def batch(self, inputs: List[Any], config: Optional[Dict] = None) -> List[Message]:
            """LCEL Runnable.batchæ–¹æ³•å®ç°"""
            return [self.invoke(input_data, config) for input_data in inputs]
    
        async def abatch(self, inputs: List[Any], config: Optional[Dict] = None) -> List[Message]:
            """LCEL Runnable.abatchå¼‚æ­¥æ‰¹å¤„ç†æ–¹æ³•å®ç°"""
            return [await self.ainvoke(input_data, config) for input_data in inputs]
    
        def get_lcel_model_output(self) -> Message:
            """è·å–LCELå…¼å®¹çš„æ¨¡å‹è¾“å‡º"""
            
            start_time = time.time()
            
            try:
                # åŠ è½½ç¯å¢ƒé…ç½®
                env_config = self._load_environment_config()
                
                # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
                routing_info = self.extract_routing_info(self.routing_context)
                memory_info = self.extract_memory_context(getattr(self, 'memory_context', None))
                route = routing_info["route"]
                confidence = routing_info["confidence"]
                has_memory = routing_info["has_memory"] or memory_info["has_memory"]
                
                has_rag_context = bool(self.extract_context_documents(getattr(self, 'context_documents', None)))
                has_tools = bool(self.extract_tools_info(getattr(self, 'tools', None)))
                
                # æ„å»ºæ¨¡å‹å’Œæ¶ˆæ¯
                model = self.build_model(env_config, routing_info)
                messages = self.build_messages(self.routing_context, getattr(self, 'memory_context', None))
                
                # è°ƒç”¨æ¨¡å‹
                response = model.invoke(messages)
                result_text = response.content
                
                execution_time = int((time.time() - start_time) * 1000)
                
                # æ„å»ºè¾“å‡ºmetadata
                output_metadata = self._create_model_metadata(
                    routing_info, memory_info, execution_time, result_text, 
                    has_rag_context, has_tools, env_config
                )
                
                # ç”ŸæˆçŠ¶æ€æŒ‡ç¤ºå™¨
                memory_indicator = "ğŸ’¬" if memory_info["format"] == "langflow_chat_memory" else ("ğŸ“" if has_memory else "ğŸ†•")
                rag_indicator = "ğŸ“š" if has_rag_context else ""
                tool_indicator = "ğŸ”§" if has_tools else ""
                param_info = f"T:{self.temperature:.2f}|P:{self.top_p:.2f}"
                
                self.status = f"âœ… {route.upper()} ({confidence:.2f}) | {execution_time}ms | {len(result_text)} chars | {param_info} {memory_indicator}{rag_indicator}{tool_indicator}"
                
                return Message(text=result_text, metadata=output_metadata)
                
            except Exception as e:
                error_time = int((time.time() - start_time) * 1000)
                self.status = f"âŒ Error: {str(e)[:50]}... | {error_time}ms"
                return Message(
                    text=f"æŠ±æ­‰ï¼Œæ¨¡å‹æ‰§è¡Œå‡ºç°é”™è¯¯ï¼š{str(e)}",
                    metadata={
                        "error": str(e), 
                        "execution_time_ms": error_time,
                        "component_source": "CustomLanguageModelComponent"
                    }
                )
    
        def _create_model_metadata(self, routing_info: Dict, memory_info: Dict,
                                  execution_time: int, result_text: str, has_rag_context: bool,
                                  has_tools: bool, env_config: Dict) -> Dict[str, Any]:
            """åˆ›å»ºæ¨¡å‹è¾“å‡ºmetadataï¼Œæ”¯æŒLCELå’ŒLangChain.jså…¼å®¹æ€§"""
            
            if self.langchainjs_export:
                # LangChain.jså…¼å®¹æ ¼å¼ï¼ˆæ‰å¹³åŒ–ï¼‰
                return {
                    # LangChain.jsæ ¸å¿ƒå­—æ®µ
                    "type": "model_output",
                    "source": "langflow",
                    "component": "CustomLanguageModelComponent",
                    "version": "1.0",
                    "timestamp": datetime.now().isoformat(),
                    
                    # æ¨¡å‹ä¿¡æ¯
                    "model_provider": self.provider,
                    "model_name": self.get_effective_model_name(),
                    "model_temperature": self.temperature,
                    "model_max_tokens": self.max_tokens,
                    "model_top_p": self.top_p,
                    "model_streaming": self.stream,
                    
                    # è·¯ç”±ä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                    "routing_route": routing_info["route"],
                    "routing_confidence": routing_info["confidence"],
                    "routing_language": routing_info["language"],
                    "routing_is_chinese": routing_info["is_chinese"],
                    "routing_analysis_method": routing_info["analysis_method"],
                    
                    # æ‰§è¡Œä¿¡æ¯
                    "execution_time_ms": execution_time,
                    "response_length": len(result_text),
                    "execution_timestamp": time.time(),
                    
                    # ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ‰å¹³åŒ–ï¼‰
                    "has_memory": memory_info["has_memory"],
                    "memory_format": memory_info["format"],
                    "has_rag_context": has_rag_context,
                    "has_tools": has_tools,
                    
                    # LangChain.jså…¼å®¹æ€§æ ‡è®°
                    "lcel_compatible": True,
                    "langchainjs_ready": True,
                    "runnable_interface": self.enable_runnable_interface,
                    "runnable_type": "language_model",
                    "supports_streaming": True,
                    "supports_async": True,
                    "supports_batching": True,
                    
                    # ç¯å¢ƒä¿¡æ¯
                    "env_mode": "vercel" if self.vercel_mode else "standard",
                    "langserve_mode": self.enable_langserve_mode,
                    "flatten_metadata": self.flatten_metadata
                }
            elif self.flatten_metadata:
                # æ‰å¹³åŒ–æ ¼å¼
                return {
                    "model_provider": self.provider,
                    "model_name": self.get_effective_model_name(),
                    "model_temperature": self.temperature,
                    "model_max_tokens": self.max_tokens,
                    "model_top_p": self.top_p,
                    "routing_route": routing_info["route"],
                    "routing_confidence": routing_info["confidence"],
                    "routing_language": routing_info["language"],
                    "routing_analysis_method": routing_info["analysis_method"],
                    "execution_time_ms": execution_time,
                    "response_length": len(result_text),
                    "has_memory": memory_info["has_memory"],
                    "memory_format": memory_info["format"],
                    "has_rag_context": has_rag_context,
                    "has_tools": has_tools,
                    "component_source": "CustomLanguageModelComponent",
                    "lcel_compatible": True,
                    "runnable_interface": self.enable_runnable_interface
                }
            else:
                # åµŒå¥—æ ¼å¼
                return {
                    "model_info": {
                        "provider": self.provider,
                        "name": self.get_effective_model_name(),
                        "parameters": {
                            "temperature": self.temperature,
                            "max_tokens": self.max_tokens,
                            "top_p": self.top_p,
                            "streaming": self.stream
                        }
                    },
                    "routing_info": routing_info,
                    "memory_info": memory_info,
                    "execution_info": {
                        "time_ms": execution_time,
                        "response_length": len(result_text),
                        "timestamp": time.time()
                    },
                    "context_info": {
                        "has_rag_context": has_rag_context,
                        "has_tools": has_tools,
                        "has_memory": memory_info["has_memory"]
                    },
                    "lcel_compatible": True,
                    "runnable_interface": self.enable_runnable_interface,
                    "component_source": "CustomLanguageModelComponent"
                }
    
        def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None) -> dotdict:
            """æ›´æ–°æ„å»ºé…ç½®ï¼Œæ”¯æŒåŠ¨æ€UIæ›´æ–°"""
            if field_name == "use_custom_model":
                if field_value:
                    build_config["model_name"]["show"] = False
                    build_config["custom_model_name"]["show"] = True
                else:
                    build_config["model_name"]["show"] = True
                    build_config["custom_model_name"]["show"] = False
                    
            elif field_name == "use_custom_url":
                if field_value:
                    build_config["base_url"]["show"] = True
                else:
                    build_config["base_url"]["show"] = False
                    
            elif field_name == "provider":
                # æ ¹æ®æä¾›å•†æ›´æ–°æ¨¡å‹é€‰é¡¹
                if field_value == "OpenAI":
                    build_config["model_name"]["options"] = ["gpt-image-1", "gpt-5", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano","o4-mini","o3-pro-all","text-embedding-3-small"]
                    build_config["openai_api_key"]["show"] = True
                    build_config["anthropic_api_key"]["show"] = False
                    build_config["google_api_key"]["show"] = False
                elif field_value == "Anthropic":
                    build_config["model_name"]["options"] = ["claude-sonnet-4-all", "claude-opus-4-thinking-all", "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite-preview-06-17"]
                    build_config["openai_api_key"]["show"] = False
                    build_config["anthropic_api_key"]["show"] = True
                    build_config["google_api_key"]["show"] = False
                elif field_value == "Google":
                    build_config["model_name"]["options"] = ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite-preview-06-17"]
                    build_config["openai_api_key"]["show"] = False
                    build_config["anthropic_api_key"]["show"] = False
                    build_config["google_api_key"]["show"] = True
                elif field_value == "DeepSeek":
                    build_config["model_name"]["options"] = ["deepseek-chat", "deepseek-reasoner"]
                    build_config["openai_api_key"]["show"] = True
                    build_config["anthropic_api_key"]["show"] = False
                    build_config["google_api_key"]["show"] = False
                    
            elif field_name == "langchainjs_export":
                if field_value:
                    build_config["enable_runnable_interface"]["show"] = True
                    build_config["flatten_metadata"]["show"] = True
                    build_config["enable_langserve_mode"]["show"] = True
                else:
                    build_config["enable_runnable_interface"]["show"] = False
                    
            elif field_name == "vercel_mode":
                if field_value:
                    build_config["enable_runnable_interface"]["value"] = True
                    build_config["flatten_metadata"]["value"] = True
                    
            return build_config
    
        # LCEL Chainç»„åˆæ–¹æ³•
        def pipe(self, *others):
            """LCEL pipeæ“ä½œç¬¦æ”¯æŒ"""
            from langchain_core.runnables import RunnableSequence
            return RunnableSequence(first=self, middle=[], last=others[0] if others else RunnablePassthrough())
        
        def __or__(self, other):
            """LCEL | æ“ä½œç¬¦æ”¯æŒ"""
            return self.pipe(other)
            
        def __ror__(self, other):
            """LCEL | æ“ä½œç¬¦æ”¯æŒï¼ˆåå‘ï¼‰"""
            if hasattr(other, 'pipe'):
                return other.pipe(self)
            from langchain_core.runnables import RunnableSequence
            return RunnableSequence(first=other, middle=[], last=self)
    
        @property
        def config_specs(self):
            """LCELé…ç½®è§„èŒƒ"""
            return {
                "runnable_type": "language_model",
                "supports_streaming": True,
                "supports_async": True,
                "supports_batching": True,
                "memory_compatible": True,
                "langchainjs_exportable": True
            }
    
        # ä¿æŒå‘åå…¼å®¹
        def get_model_output(self) -> Message:
            """ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•å"""
            return self.get_lcel_model_output()
        # åœ¨ä½ çš„ç»„ä»¶ä¸­æ·»åŠ memoryå¤„ç†æ–¹æ³•
        def format_memory_for_export(self, memory_message):
            """æ ¼å¼åŒ–memoryä¸ºå¯¼å‡ºå‹å¥½çš„æ ¼å¼"""
            if not memory_message:
                return None
    
            # æå–ChatMemoryå†…å®¹
            memory_content = memory_message.text if hasattr(memory_message, 'text') else str(memory_message)
    
            # è§£æå¯¹è¯å†å²
            conversations = []
            lines = memory_content.split('\n')
            for line in lines:
                if line.startswith('Human:') or line.startswith('User:'):
                    conversations.append({"role": "user", "content": line.split(':', 1)[1].strip()})
                elif line.startswith('Assistant:') or line.startswith('AI:'):
                    conversations.append({"role": "assistant", "content": line.split(':', 1)[1].strip()})
    
            return {
                "type": "chat_memory",
                "messages": conversations[-10:],  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
                "format": "langchain_compatible"
            }
        # ç»Ÿä¸€çš„memory metadataæ ¼å¼
        def create_memory_metadata(self, memory_data):
            return {
                "memory": {
                    "has_memory": bool(memory_data),
                    "format": "langflow_chat_memory",
                    "messages": memory_data.get("messages", []) if memory_data else [],
                    "export_ready": True
                },
                "langchain_export": {
                    "memory_compatible": True,
                    "vercel_ready": True
                }
            }
    


â€‹    